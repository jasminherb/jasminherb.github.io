---
layout: single
title:  "KNN (27) "
---


# Machine Learning - KNN & StandardScaler

출처 : Thebook.io (모두의 딥러닝 & 머신러닝 워크북)

K-최근접 이웃(K-Nearest Neighbors, KNN)은 지도 학습(Supervised Learning) 알고리즘 중 하나로, 
분류(Classification)와 회귀(Regression) 문제에 사용됩니다. 

KNN은 인스턴스 기반(Instance-based) 학습 알고리즘으로도 알려져 있습니다.

KNN의 작동 방식은 매우 간단합니다. 
주어진 데이터셋 내에서 새로운 데이터 포인트의 이웃을 찾아 분류하거나 예측합니다. 
여기서 "이웃"은 주어진 데이터 공간에서 가장 가까운 데이터 포인트들을 의미합니다. 

KNN은 데이터셋 내에서 가장 가까운 K개의 이웃을 선택하여 이들의 다수결 또는 평균 값을 기반으로 
새로운 데이터 포인트를 분류하거나 예측합니다.



KNN 알고리즘의 주요 단계는 다음과 같습니다:

데이터셋 준비: 분류 또는 회귀를 위한 훈련 데이터셋을 준비합니다. 
각 데이터 포인트는 특징(Feature)으로 이루어진 벡터로 표현됩니다.

거리 측정: 새로운 데이터 포인트와 훈련 데이터셋 내의 모든 데이터 포인트 간의 거리를 계산합니다. 
일반적으로 유클리드 거리(Euclidean distance)가 가장 많이 사용되지만, 다른 거리 측정 방법도 사용할 수 있습니다.

이웃 선택: 계산된 거리를 기준으로 가장 가까운 K개의 이웃을 선택합니다.

다수결 또는 평균: 분류 문제의 경우, 선택된 이웃들의 다수결을 통해 새로운 데이터 포인트의 클래스를 결정합니다. 
회귀 문제의 경우, 선택된 이웃들의 평균 값을 예측 결과로 사용합니다.



KNN의 장점은 다음과 같습니다:

간단하고 직관적인 알고리즘으로 이해하기 쉽습니다.
새로운 데이터 포인트에 대한 예측이 빠릅니다.
학습 데이터에 대한 가정이 거의 없기 때문에 다양한 유형의 데이터에 적용할 수 있습니다.





하지만 KNN은 몇 가지 주요 단점도 가지고 있습니다:

모든 훈련 데이터셋을 메모리에 유지해야 하므로, 큰 규모의 데이터셋에는 적합하지 않을 수 있습니다.
거리 기반 알고리즘이기 때문에 차원이 매우 높은 데이터에서는 성능이 저하될 가능성 있습니다.

```python

#모든 행을 사용하지만 열(칼럼)은 뒤에서 하나를 뺀 값을 가져와서 X에 저장
X = df.iloc[:, :-1].values 

#모든 행을 사용하지만 열은 앞에서 다섯 번째 값만 가져와서 y에 저장
y = df.iloc[:, 4].values 

#X, y를 사용하여 훈련과 검증 데이터셋으로 분리하며, 검증 세트의 비율은 20%만 사용
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) 

# 전처리 (정규화)
from sklearn.preprocessing import StandardScaler

s = StandardScaler()
X_train = s.fit(X_train).transform(X_train) 
X_test = s.fit(X_test).transform(X_test)

#KNN 구별
from sklearn.neighbors import KNeighborsClassifier

#K=20인 K-최근접 이웃 모델 생성
knn = KNeighborsClassifier(n_neighbors=20) 

#모델 훈련
knn.fit(X_train, y_train) 

from sklearn.metrics import accuracy_score

y_pred = knn.predict(X_test)
print("정확도: {}".format(accuracy_score(y_test, y_pred)))



#for 문을 이용하여 K 값을 1부터 10까지 순환하면서 최적의 K 값과 정확도를 찾습니다.

k = 10
acc_array = np.zeros(k)
for k in np.arange(1, k+1, 1): #K는 1에서 10까지 값을 취함
    classifier = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train) #for 문을 반복하면서 K 값 변경
    y_pred = classifier.predict(X_test)
    acc = metrics.accuracy_score(y_test, y_pred)
    acc_array[k-1] = acc

max_acc = np.amax(acc_array)
acc_list = list(acc_array)
k = acc_list.index(max_acc)
print("정확도", max_acc, "으로 최적의 k는", k+1, "입니다.")
```

