---
layout: single
title:  "EarlyStopping Callback & Checkpointer(9) "
---

# 주택가격 예측

출처 : Thebook.io (모두의 딥러닝)

주택 가격 예측하기

주거 유형, 차고, 자재 및 환경에 관한 80개의 서로 다른 속성을 이용해 집의 가격을 예측해 볼 예정인데 

오랜 시간 사람이 일일이 기록하다 보니 빠진 부분도 많고,

집에 따라 어떤 항목은 범위에서 너무 벗어나 있기도 하며, 

또 가격과는 관계가 없는 정보가 포함되어 있기도 합니다. 

실제 현장에서 만나게 되는 이런 류의 데이터를 어떻게 다루어야 하는지 보겠습니다.



![street-5932230_640](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/a48364ae-bf44-4f88-be28-cecc4fcfbb31)


데이터 Type 파악하기

```python
import pandas as pd

# 집 값 데이터를 불러옵니다.
df = pd.read_csv("./data/house_train.csv")

df

df.dtypes

```

1460 rows × 81 columns


Id int64

MSSubClass         int64

MSZoning          object

LotFrontage      float64

LotArea            int64
          
MoSold             int64

YrSold             int64

SaleType          object

SaleCondition     object

SalePrice          int64

Length: 81, dtype: object







결측치가 있는지 알아보는 함수는 isnull()입니다. 

결측치가 모두 몇 개인지 세어 가장 많은 것부터 순서대로 나열한 후 처음 20개만 출력하는 코드는 다음과 같습니다.


```python

df.isnull().sum().sort_values(ascending=False).head(20)

```
![image](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/4ef45033-8455-4b4d-9f59-2b39a274d0d2)


결측치가 많은 항목은 1,460개의 샘플 중에서 1,453개나 비어 있을 만큼 빠진 곳이 많은 것을 확인할 수 있습니다.

get_dummies() 함수를 이용해 카테고리형 변수를 0과 1로 이루어진 변수로 바꾸어 줍니다.

```python
df = pd.get_dummies(df)
```

그리고 결측치를 채워 줍니다. 

결측치를 채워 주는 함수는 판다스의 fillna()입니다. 괄호 안에 df.mean()을 넣어 주면 평균값으로 채워 줍니다.

```python
df = df.fillna(df.mean())
```


이 중에서 우리에게 필요한 정보를 추출해 보겠습니다. 

먼저 데이터 사이의 상관관계를 df_corr 변수에 저장합니다. 

그리고  집 값과 관련이 큰 것부터 순서대로 정렬해 df_corr_ sort 변수에 저장합니다.  

집 값과 관련도가 가장 큰 열 개의 속성들을 출력합니다.


```python

df_corr = df.corr() 
df_corr_sort = df_corr.sort_values('SalePrice', ascending=False)
df_corr_sort['SalePrice'].head(10)

```
![image](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/ecfcd6ac-e1c3-410e-a5e6-9873bfc153fd)



추출된 속성들과 집 값의 관련도를 시각적으로 확인하기 위해 상관도 그래프를 그려 보겠습니다.

```python
cols = ['SalePrice','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF']
sns.pairplot(df[cols])
plt.show();

```

![housing](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/3d0fb47e-c6e7-4100-8fb7-bd58a9e1122d)




```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np

# 깃허브에 준비된 데이터를 가져옵니다.
!git clone https://github.com/taehojo/data.git

# 집 값 데이터를 불러옵니다.
df = pd.read_csv("./data/house_train.csv")

# 카테고리형 변수를 0과 1로 이루어진 변수로 바꾸어 줍니다.
df = pd.get_dummies(df)

# 결측치를 전체 칼럼의 평균으로 대체해 채워 줍니다.
df = df.fillna(df.mean())

# 데이터 사이의 상관관계를 저장합니다.
df_corr = df.corr()

# 집 값과 관련이 큰 것부터 순서대로 저장합니다.
df_corr_sort = df_corr.sort_values('SalePrice', ascending=False)

# 집 값을 제외한 나머지 열을 저장합니다. 
cols_train = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF']
X_train_pre = df[cols_train]

# 집 값을 저장합니다.
y = df['SalePrice'].values

# 전체의 80%를 학습셋으로, 20%를 테스트셋으로 지정합니다.
X_train, X_test, y_train, y_test = train_test_split(X_train_pre, y, test_size=0.2)

# 모델의 구조를 설정합니다.
model = Sequential()
model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(40, activation='relu'))
model.add(Dense(1))
model.summary()

# 모델을 실행합니다.
model.compile(optimizer='adam', loss='mean_squared_error')

# 20번 이상 결과가 향상되지 않으면 자동으로 중단되게끔 합니다.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)

# 모델의 이름을 정합니다.
modelpath = "./data/model/Ch15-house.hdf5"

# 최적화 모델을 업데이트하고 저장합니다.
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)

# 실행 관련 설정을 하는 부분입니다. 전체의 20%를 검증셋으로 설정합니다.
history = model.fit(X_train, y_train, validation_split=0.25, epochs=2000, batch_size=32, callbacks=[early_stopping_callback,checkpointer])

real_prices = []
pred_prices = []
X_num = []

n_iter = 0
Y_prediction = model.predict(X_test).flatten()
for i in range(25):
    real = y_test[i]
    prediction = Y_prediction[i]
    print("실제가격: {:.2f}, 예상가격: {:.2f}".format(real, prediction))
    real_prices.append(real)
    pred_prices.append(prediction)
    n_iter = n_iter + 1
    X_num.append(n_iter)
    
    
plt.plot(X_num, pred_prices, label='predicted price')
plt.plot(X_num, real_prices, label='real price')
plt.legend()
plt.show()    
```

#### 결과표시

Cloning into 'data'...
remote: Enumerating objects: 21, done.
remote: Counting objects: 100% (21/21), done.
remote: Compressing objects: 100% (18/18), done.
remote: Total 21 (delta 3), reused 20 (delta 2), pack-reused 0
Unpacking objects: 100% (21/21), 460.93 KiB | 1.52 MiB/s, done.
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param   
=================================================================
 dense_2 (Dense)             (None, 10)                60        
                                                                 
 dense_3 (Dense)             (None, 30)                330       
                                                                 
 dense_4 (Dense)             (None, 40)                1240      
                                                                 
 dense_5 (Dense)             (None, 1)                 41        
                                                                 
=================================================================
Total params: 1,671
Trainable params: 1,671
Non-trainable params: 0
_________________________________________________________________
Epoch 1/2000
28/28 [==============================] - 5s 25ms/step - loss: 38155685888.0000 - val_loss: 40911536128.0000
Epoch 2/2000
28/28 [==============================] - 0s 14ms/step - loss: 37793415168.0000 - val_loss: 40379387904.0000
Epoch 3/2000
28/28 [==============================] - 1s 24ms/step - loss: 37068910592.0000 - val_loss: 39173341184.0000
Epoch 4/2000
28/28 [==============================] - 0s 15ms/step - loss: 35367882752.0000 - val_loss: 36362940416.0000
Epoch 5/2000
28/28 [==============================] - 0s 15ms/step - loss: 31532853248.0000 - val_loss: 30409005056.0000
Epoch 6/2000
28/28 [==============================] - 1s 19ms/step - loss: 24138172416.0000 - val_loss: 20300537856.0000
Epoch 7/2000
28/28 [==============================] - 1s 28ms/step - loss: 13629459456.0000 - val_loss: 8181811200.0000
Epoch 8/2000
28/28 [==============================] - 0s 16ms/step - loss: 4571030528.0000 - val_loss: 2026180992.0000
Epoch 9/2000
28/28 [==============================] - 1s 19ms/step - loss: 2138632576.0000 - val_loss: 1576219776.0000
Epoch 10/2000
28/28 [==============================] - 0s 14ms/step - loss: 2075046784.0000 - val_loss: 1578211072.0000
Epoch 11/2000
28/28 [==============================] - 0s 14ms/step - loss: 2065400320.0000 - val_loss: 1578315008.0000
Epoch 12/2000
28/28 [==============================] - 1s 20ms/step - loss: 2059927424.0000 - val_loss: 1568391040.0000
Epoch 13/2000
28/28 [==============================] - 0s 16ms/step - loss: 2061175168.0000 - val_loss: 1570747136.0000
Epoch 14/2000
28/28 [==============================] - 0s 12ms/step - loss: 2064374912.0000 - val_loss: 1568311552.0000
Epoch 15/2000
28/28 [==============================] - 0s 9ms/step - loss: 2062475392.0000 - val_loss: 1567785984.0000
Epoch 16/2000
28/28 [==============================] - 0s 8ms/step - loss: 2061223552.0000 - val_loss: 1556778496.0000
Epoch 17/2000
28/28 [==============================] - 0s 9ms/step - loss: 2053697792.0000 - val_loss: 1564341376.0000
Epoch 18/2000
28/28 [==============================] - 0s 9ms/step - loss: 2061399168.0000 - val_loss: 1564453376.0000
Epoch 19/2000
28/28 [==============================] - 0s 9ms/step - loss: 2057601920.0000 - val_loss: 1560138752.0000
Epoch 20/2000
28/28 [==============================] - 0s 10ms/step - loss: 2042568448.0000 - val_loss: 1556015488.0000
Epoch 21/2000
28/28 [==============================] - 0s 9ms/step - loss: 2052477696.0000 - val_loss: 1559030400.0000
Epoch 22/2000
28/28 [==============================] - 0s 8ms/step - loss: 2044059904.0000 - val_loss: 1544153984.0000
Epoch 23/2000
28/28 [==============================] - 0s 6ms/step - loss: 2033061120.0000 - val_loss: 1554104704.0000
Epoch 24/2000
28/28 [==============================] - 0s 5ms/step - loss: 2036587008.0000 - val_loss: 1538330496.0000
Epoch 25/2000
28/28 [==============================] - 0s 5ms/step - loss: 2028587520.0000 - val_loss: 1546157440.0000
Epoch 26/2000
28/28 [==============================] - 0s 5ms/step - loss: 2026864128.0000 - val_loss: 1547168128.0000
Epoch 27/2000
28/28 [==============================] - 0s 5ms/step - loss: 2027914240.0000 - val_loss: 1542044800.0000
Epoch 28/2000
28/28 [==============================] - 0s 6ms/step - loss: 2021309440.0000 - val_loss: 1533817728.0000
Epoch 29/2000
28/28 [==============================] - 0s 5ms/step - loss: 2020437632.0000 - val_loss: 1543653120.0000
Epoch 30/2000
28/28 [==============================] - 0s 6ms/step - loss: 2019305088.0000 - val_loss: 1532642304.0000
Epoch 31/2000
28/28 [==============================] - 0s 5ms/step - loss: 2018277888.0000 - val_loss: 1542835328.0000
Epoch 32/2000
28/28 [==============================] - 0s 7ms/step - loss: 2028840448.0000 - val_loss: 1527085056.0000
Epoch 33/2000
28/28 [==============================] - 0s 5ms/step - loss: 2020086784.0000 - val_loss: 1527812480.0000
Epoch 34/2000
28/28 [==============================] - 0s 5ms/step - loss: 2010653312.0000 - val_loss: 1528218368.0000
Epoch 35/2000
28/28 [==============================] - 0s 5ms/step - loss: 2009069056.0000 - val_loss: 1536035968.0000
Epoch 36/2000
28/28 [==============================] - 0s 6ms/step - loss: 2009024768.0000 - val_loss: 1524913152.0000
Epoch 37/2000
28/28 [==============================] - 0s 6ms/step - loss: 2005762816.0000 - val_loss: 1523899136.0000
Epoch 38/2000
28/28 [==============================] - 0s 6ms/step - loss: 2006764800.0000 - val_loss: 1519990784.0000
Epoch 39/2000
28/28 [==============================] - 0s 5ms/step - loss: 2031107072.0000 - val_loss: 1538248576.0000
Epoch 40/2000
28/28 [==============================] - 0s 5ms/step - loss: 2004483200.0000 - val_loss: 1525601024.0000
Epoch 41/2000
28/28 [==============================] - 0s 5ms/step - loss: 2005841664.0000 - val_loss: 1525578880.0000
Epoch 42/2000
28/28 [==============================] - 0s 5ms/step - loss: 2005777920.0000 - val_loss: 1524727296.0000
Epoch 43/2000
28/28 [==============================] - 0s 5ms/step - loss: 2010332928.0000 - val_loss: 1529280768.0000
Epoch 44/2000
28/28 [==============================] - 0s 5ms/step - loss: 1997195136.0000 - val_loss: 1526140032.0000
Epoch 45/2000
28/28 [==============================] - 0s 5ms/step - loss: 2009581184.0000 - val_loss: 1528287488.0000
Epoch 46/2000
28/28 [==============================] - 0s 5ms/step - loss: 1999578496.0000 - val_loss: 1528665728.0000
Epoch 47/2000
28/28 [==============================] - 0s 7ms/step - loss: 1997539584.0000 - val_loss: 1509591680.0000
Epoch 48/2000
28/28 [==============================] - 0s 5ms/step - loss: 1991906688.0000 - val_loss: 1539109248.0000
Epoch 49/2000
28/28 [==============================] - 0s 6ms/step - loss: 2005767680.0000 - val_loss: 1504359808.0000
Epoch 50/2000
28/28 [==============================] - 0s 4ms/step - loss: 1998059136.0000 - val_loss: 1513588096.0000
Epoch 51/2000
28/28 [==============================] - 0s 5ms/step - loss: 1990022528.0000 - val_loss: 1507888384.0000
Epoch 52/2000
28/28 [==============================] - 0s 5ms/step - loss: 2001906304.0000 - val_loss: 1513538304.0000
Epoch 53/2000
28/28 [==============================] - 0s 5ms/step - loss: 1986248960.0000 - val_loss: 1511896448.0000
Epoch 54/2000
28/28 [==============================] - 0s 6ms/step - loss: 1998663168.0000 - val_loss: 1504111872.0000
Epoch 55/2000
28/28 [==============================] - 0s 5ms/step - loss: 2003390592.0000 - val_loss: 1529675264.0000
Epoch 56/2000
28/28 [==============================] - 0s 5ms/step - loss: 2002062464.0000 - val_loss: 1525011328.0000
Epoch 57/2000
28/28 [==============================] - 0s 5ms/step - loss: 1986290944.0000 - val_loss: 1507372160.0000
Epoch 58/2000
28/28 [==============================] - 0s 5ms/step - loss: 1986825472.0000 - val_loss: 1518103424.0000
Epoch 59/2000
28/28 [==============================] - 0s 4ms/step - loss: 1994590976.0000 - val_loss: 1521247616.0000
Epoch 60/2000
28/28 [==============================] - 0s 5ms/step - loss: 1989974656.0000 - val_loss: 1512253824.0000
Epoch 61/2000
28/28 [==============================] - 0s 5ms/step - loss: 1989538176.0000 - val_loss: 1520689664.0000
Epoch 62/2000
28/28 [==============================] - 0s 5ms/step - loss: 1991334912.0000 - val_loss: 1531031168.0000
Epoch 63/2000
28/28 [==============================] - 0s 5ms/step - loss: 1985658752.0000 - val_loss: 1514404992.0000
Epoch 64/2000
28/28 [==============================] - 0s 5ms/step - loss: 1982932480.0000 - val_loss: 1513915136.0000
Epoch 65/2000
28/28 [==============================] - 0s 5ms/step - loss: 1983147008.0000 - val_loss: 1506508288.0000
Epoch 66/2000
28/28 [==============================] - 0s 5ms/step - loss: 1981682304.0000 - val_loss: 1519915392.0000
Epoch 67/2000
28/28 [==============================] - 0s 7ms/step - loss: 1987618560.0000 - val_loss: 1503814912.0000
Epoch 68/2000
28/28 [==============================] - 0s 5ms/step - loss: 1996779776.0000 - val_loss: 1531592064.0000
Epoch 69/2000
28/28 [==============================] - 0s 6ms/step - loss: 1991065216.0000 - val_loss: 1499786624.0000
Epoch 70/2000
28/28 [==============================] - 0s 5ms/step - loss: 1988536704.0000 - val_loss: 1524427776.0000
Epoch 71/2000
28/28 [==============================] - 0s 5ms/step - loss: 1978326016.0000 - val_loss: 1500418048.0000
Epoch 72/2000
28/28 [==============================] - 0s 5ms/step - loss: 1978897536.0000 - val_loss: 1513844096.0000
Epoch 73/2000
28/28 [==============================] - 0s 5ms/step - loss: 1981214208.0000 - val_loss: 1525452800.0000
Epoch 74/2000
28/28 [==============================] - 0s 6ms/step - loss: 1990907008.0000 - val_loss: 1504270592.0000
Epoch 75/2000
28/28 [==============================] - 0s 5ms/step - loss: 1985649024.0000 - val_loss: 1509634944.0000
Epoch 76/2000
28/28 [==============================] - 0s 4ms/step - loss: 1987153408.0000 - val_loss: 1509752320.0000
Epoch 77/2000
28/28 [==============================] - 0s 6ms/step - loss: 1986407552.0000 - val_loss: 1516328704.0000
Epoch 78/2000
28/28 [==============================] - 0s 5ms/step - loss: 1991560576.0000 - val_loss: 1543298560.0000
Epoch 79/2000
28/28 [==============================] - 0s 5ms/step - loss: 1990822528.0000 - val_loss: 1496584576.0000
Epoch 80/2000
28/28 [==============================] - 0s 5ms/step - loss: 1986133888.0000 - val_loss: 1513832064.0000
Epoch 81/2000
28/28 [==============================] - 0s 5ms/step - loss: 1988901248.0000 - val_loss: 1507190912.0000
Epoch 82/2000
28/28 [==============================] - 0s 5ms/step - loss: 1984330240.0000 - val_loss: 1510425856.0000
Epoch 83/2000
28/28 [==============================] - 0s 5ms/step - loss: 1976945152.0000 - val_loss: 1497728896.0000
Epoch 84/2000
28/28 [==============================] - 0s 5ms/step - loss: 1985533568.0000 - val_loss: 1501458304.0000
Epoch 85/2000
28/28 [==============================] - 0s 5ms/step - loss: 1982763392.0000 - val_loss: 1536504064.0000
Epoch 86/2000
28/28 [==============================] - 0s 4ms/step - loss: 1976697984.0000 - val_loss: 1510464896.0000
Epoch 87/2000
28/28 [==============================] - 0s 5ms/step - loss: 1976950400.0000 - val_loss: 1503840512.0000
Epoch 88/2000
28/28 [==============================] - 0s 5ms/step - loss: 1982659712.0000 - val_loss: 1554420608.0000
Epoch 89/2000
28/28 [==============================] - 0s 5ms/step - loss: 1995554432.0000 - val_loss: 1499632384.0000
Epoch 90/2000
28/28 [==============================] - 0s 8ms/step - loss: 1980673024.0000 - val_loss: 1533995264.0000
Epoch 91/2000
28/28 [==============================] - 0s 8ms/step - loss: 1992144768.0000 - val_loss: 1509249408.0000
Epoch 92/2000
28/28 [==============================] - 0s 8ms/step - loss: 1977486592.0000 - val_loss: 1507465856.0000
Epoch 93/2000
28/28 [==============================] - 0s 8ms/step - loss: 1978399616.0000 - val_loss: 1509043200.0000
Epoch 94/2000
28/28 [==============================] - 0s 9ms/step - loss: 1977654016.0000 - val_loss: 1496408832.0000
Epoch 95/2000
28/28 [==============================] - 0s 8ms/step - loss: 1976178048.0000 - val_loss: 1517755008.0000
Epoch 96/2000
28/28 [==============================] - 0s 8ms/step - loss: 1974451584.0000 - val_loss: 1507079552.0000
Epoch 97/2000
28/28 [==============================] - 0s 9ms/step - loss: 1974260608.0000 - val_loss: 1512951168.0000
Epoch 98/2000
28/28 [==============================] - 0s 7ms/step - loss: 1974503296.0000 - val_loss: 1500129536.0000
Epoch 99/2000
28/28 [==============================] - 0s 9ms/step - loss: 1996331008.0000 - val_loss: 1553896704.0000
Epoch 100/2000
28/28 [==============================] - 0s 10ms/step - loss: 1974382336.0000 - val_loss: 1493927680.0000
Epoch 101/2000
28/28 [==============================] - 0s 8ms/step - loss: 1980366080.0000 - val_loss: 1528402816.0000
Epoch 102/2000
28/28 [==============================] - 0s 8ms/step - loss: 1990248576.0000 - val_loss: 1510623104.0000
Epoch 103/2000
28/28 [==============================] - 0s 7ms/step - loss: 1975655936.0000 - val_loss: 1518400128.0000
Epoch 104/2000
28/28 [==============================] - 0s 8ms/step - loss: 1975495936.0000 - val_loss: 1501774336.0000
Epoch 105/2000
28/28 [==============================] - 0s 5ms/step - loss: 1981282944.0000 - val_loss: 1530294016.0000
Epoch 106/2000
28/28 [==============================] - 0s 4ms/step - loss: 1994542976.0000 - val_loss: 1529674880.0000
Epoch 107/2000
28/28 [==============================] - 0s 5ms/step - loss: 1983368832.0000 - val_loss: 1517283584.0000
Epoch 108/2000
28/28 [==============================] - 0s 5ms/step - loss: 1984388608.0000 - val_loss: 1534877312.0000
Epoch 109/2000
28/28 [==============================] - 0s 5ms/step - loss: 1976305280.0000 - val_loss: 1504436096.0000
Epoch 110/2000
28/28 [==============================] - 0s 4ms/step - loss: 1980761856.0000 - val_loss: 1497187200.0000
Epoch 111/2000
28/28 [==============================] - 0s 5ms/step - loss: 1984798592.0000 - val_loss: 1496664576.0000
Epoch 112/2000
28/28 [==============================] - 0s 5ms/step - loss: 1979922048.0000 - val_loss: 1538794112.0000
Epoch 113/2000
28/28 [==============================] - 0s 5ms/step - loss: 1979074048.0000 - val_loss: 1496132096.0000
Epoch 114/2000
28/28 [==============================] - 0s 5ms/step - loss: 1975978240.0000 - val_loss: 1508604544.0000
Epoch 115/2000
28/28 [==============================] - 0s 5ms/step - loss: 1978917632.0000 - val_loss: 1527774080.0000
Epoch 116/2000
28/28 [==============================] - 0s 5ms/step - loss: 1989198464.0000 - val_loss: 1511075200.0000
Epoch 117/2000
28/28 [==============================] - 0s 5ms/step - loss: 1985042944.0000 - val_loss: 1498298112.0000
Epoch 118/2000
28/28 [==============================] - 0s 4ms/step - loss: 1971759616.0000 - val_loss: 1513651584.0000
Epoch 119/2000
28/28 [==============================] - 0s 4ms/step - loss: 1972376832.0000 - val_loss: 1515083136.0000
Epoch 120/2000
28/28 [==============================] - 0s 5ms/step - loss: 1971113344.0000 - val_loss: 1526587264.0000
10/10 [==============================] - 0s 2ms/step

실제가격: 100000.00, 예상가격: 146393.59


실제가격: 168500.00, 예상가격: 164703.88

실제가격: 207500.00, 예상가격: 231540.70

실제가격: 155000.00, 예상가격: 156263.78

실제가격: 485000.00, 예상가격: 350086.53

실제가격: 145000.00, 예상가격: 166363.98

실제가격: 164700.00, 예상가격: 151135.23

실제가격: 191000.00, 예상가격: 189105.55

실제가격: 165600.00, 예상가격: 159426.14

실제가격: 128950.00, 예상가격: 117329.49

실제가격: 106500.00, 예상가격: 130667.62

실제가격: 181500.00, 예상가격: 175930.89

실제가격: 261500.00, 예상가격: 219995.28

실제가격: 150500.00, 예상가격: 154388.84

실제가격: 176000.00, 예상가격: 225568.52

실제가격: 151000.00, 예상가격: 142734.27

실제가격: 110000.00, 예상가격: 259534.23

실제가격: 89000.00, 예상가격: 84370.30

실제가격: 131500.00, 예상가격: 156068.53

실제가격: 144000.00, 예상가격: 128896.88

실제가격: 127000.00, 예상가격: 134261.02

실제가격: 119500.00, 예상가격: 126499.21

실제가격: 136000.00, 예상가격: 148028.47

실제가격: 305900.00, 예상가격: 292832.66

실제가격: 109500.00, 예상가격: 123058.59


![22](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/ddaaa17b-c0d8-40cf-b1ca-aaf171ff7d5e)

