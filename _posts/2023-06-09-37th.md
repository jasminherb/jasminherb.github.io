---
layout: single
title:  "Mixture Density Network "
---

#### 출처 : https://kangbk0120.github.io/articles/2018-05/MDN

현실에 있는 복잡한 형태의 확률 분포를 여러 분포 n개를 혼합하여 표현하자는 것이 MDN입니다. 
이때 n은 하이퍼파라미터입니다.

```python
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
```

from torch.autograd import Variable는 파이토치(PyTorch)에서 자동 미분(automatic differentiation)을 지원하는 모듈 중 하나입니다. 

Variable은 텐서(Tensor)를 감싸는 래퍼(wrapper) 역할을 하며, 텐서에 대한 연산을 추적하고, 그래디언트(gradient)를 계산할 수 있습니다. 

이를 통해 파이토치에서 뉴럴 네트워크(Neural Network) 모델을 학습할 때, 역전파(backpropagation) 알고리즘을 사용하여 모델의 가중치(weight)를 업데이트할 수 있습니다. 

Variable은 파이토치 0.4.0 버전 이후로는 Tensor로 통합되어 더 이상 사용되지 않습니다. 

대신에, Tensor에 requires_grad=True를 설정하여 그래디언트를 계산할 수 있습니다.


```python
def generate_data(n_samples):
    epsilon = np.random.normal(size=(n_samples))
    x_data = np.random.uniform(-10.5, 10.5, n_samples)
    y_data = 7*np.sin(0.75*x_data) + 0.5*x_data + epsilon
    return x_data, y_data

n_samples = 1000
x_data, y_data = generate_data(n_samples)
```

NumPy를 사용하여 랜덤한 값을 생성하는 코드입니다.

np.random.normal(size=(n_samples)): 정규 분포(Normal Distribution)를 따르는 랜덤한 값을 생성합니다.

size=(n_samples)는 생성할 값의 개수를 의미합니다. 

따라서 epsilon은 n_samples 개수만큼의 랜덤한 값을 가지는 1차원 배열입니다.



np.random.uniform(-10.5, 10.5, n_samples): 균등 분포(Uniform Distribution)를 따르는 랜덤한 값을 생성합니다. 

-10.5와 10.5 사이에서 n_samples 개수만큼의 랜덤한 값을 생성합니다. 

따라서 x_data는 n_samples 개수만큼의 랜덤한 값을 가지는 1차원 배열입니다.


```python
plt.figure(figsize=(8, 8))
plt.scatter(x_data, y_data, alpha=0.2)
plt.show()
```


```python
n_input = 1
n_hidden = 20
n_output = 1

network = nn.Sequential(nn.Linear(n_input, n_hidden),
                        nn.Tanh(),
                        nn.Linear(n_hidden, n_output))
loss_fn = nn.MSELoss()
```

파이토치(PyTorch)에서 신경망(Neural Network) 모델을 정의하는 코드입니다.

n_input, n_hidden, n_output: 입력층, 은닉층, 출력층의 노드 수를 의미합니다.

nn.Sequential: 순차적으로 레이어를 쌓아 신경망 모델을 만드는 함수입니다.


nn.Linear(n_input, n_hidden): 입력층과 은닉층 사이에 있는 선형 레이어(linear layer)입니다.

n_input은 입력층의 노드 수, n_hidden은 은닉층의 노드 수를 의미합니다.

nn.Tanh(): 은닉층에서 사용되는 활성화 함수(activation function)로 하이퍼볼릭 탄젠트(tanh) 함수를 사용합니다.



nn.Linear(n_hidden, n_output): 은닉층과 출력층 사이에 있는 선형 레이어입니다. 

n_hidden은 은닉층의 노드 수, n_output은 출력층의 노드 수를 의미합니다.



loss_fn = nn.MSELoss(): 평균 제곱 오차(Mean Squared Error)를 계산하는 함수입니다. 

이 함수는 신경망 모델이 예측한 값과 실제 값 사이의 차이를 계산하여 오차를 구합니다.


```python
x_tensor = torch.from_numpy(np.float32(x_data).reshape(n_samples, n_input))
y_tensor = torch.from_numpy(np.float32(y_data).reshape(n_samples, n_input))
x_variable = Variable(x_tensor)
y_variable = Variable(y_tensor, requires_grad=False)
```


numpy array를 pytorch가 사용할 수 있는 tensor로 바꿔줘야합니다. 

또한 numpy의 기본 형태인 np.float64를 pytorch의 기본형인 np.float32로 바꿔줘야합니다.

1000개의 샘플을 한번에 처리하기 위해서, 형태를 [1000, 1]로 바꿔줍니다.

```python
def train():
    for epoch in range(3000):
        y_pred = network(x_variable)
        loss = loss_fn(y_pred, y_variable)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch % 300 == 0:
            print(epoch, loss.data[0])

train()
```

네트워크에 x를 넣어 순전파를 시킨다음, 이에 대응하는 loss를 구하고, 이를 역전파시키고, 파라미터를 갱신합니다.



```python
x_test_data = np.linspace(-10, 10, n_samples)

x_test_tensor = torch.from_numpy(np.float32(x_test_data).reshape(n_samples, n_input))
x_test_variable = Variable(x_test_tensor)
y_test_variable = network(x_test_variable)

y_test_data = y_test_variable.data.numpy()

plt.figure(figsize=(8, 8))
plt.scatter(x_data, y_data, alpha=0.2)
plt.scatter(x_test_data, y_test_data, alpha=0.2)
plt.show()

```

학습 데이터외의 x에 대해서 모델이 어떻게 예측하고 있는지를 알아봅시다. 
똑같이 x를 샘플링한다음 이를 네트워크에 넣어 y값들을 얻어낸다음, 이를 다시 그래프로 표현하면 됩니다.

네트워크가 이 함수를 아주 잘 표현하고 있다는 것을 알 수 있습니다. 
이는 Universal approximation theorem이라는 이론과 관련되어 있습니다. 
이론 상으로는 은닉층 하나만으로도 Multi-Layer Perceptron은 임의의 연속함수를 근사할 수 있습니다. 


지금까지는 한 개의 mode를 갖는 정규분포에서 회귀를 했다고 생각할 수 있습니다. 
하지만 만약 여러 개의 mode로 표현되는 정규분포에서 전과 똑같이 네트워크를 학습시킨다면 어떻게 될까요? 
곧 multimodal regression을 기존의 네트워크를 가지고 해보겠다는 겁니다.


```
Multimodal regression은 입력 변수와 출력 변수 간의 다중 모드(multi-modal) 관계를 모델링하는 회귀 분석 기법입니다. 
이는 입력 변수가 동일하더라도 출력 변수가 여러 개의 값으로 나타날 수 있는 경우에 유용합니다.


예를 들어, 집의 크기와 가격 사이의 관계를 모델링한다고 가정해 봅시다. 
집의 크기가 같더라도 위치, 시장 상황, 건물 연식 등에 따라 가격이 다양하게 나타날 수 있습니다. 
이러한 경우, 단일 모델로는 모든 경우를 충분히 설명할 수 없습니다. 
따라서, multimodal regression은 이러한 다중 모드 관계를 모델링하여 보다 정확한 예측을 가능하게 합니다.


Multimodal regression은 다양한 분야에서 활용되고 있습니다.
예를 들어, 의료 분야에서는 환자의 특성과 질병 발생 확률 사이의 관계를 모델링할 때 유용하게 사용됩니다. 
또한, 금융 분야에서는 주식 가격과 관련된 여러 요인들 간의 관계를 모델링할 때 사용됩니다.
```

여러 개의 mode로 표현되는 정규분포에서 전과 똑같이 네트워크를 학습시킨다면 어떻게 될까요? 
곧 multimodal regression을 기존의 네트워크를 가지고 해보겠다는 겁니다.

```python
plt.figure(figsize=(8, 8))
plt.scatter(y_data, x_data, alpha=0.2)
plt.show()
```

입력 x가 주어졌을 떄 여러 개의 y가 가능하다는 겁니다. 
곧 이전의 예시처럼 하나의 정규분포로 표현되는 상황을 넘어, 여러 개의 정규분포를 갖는 상황이 된거죠. 
이런 상황에서 기존의 네트워크를 학습시키고 결과를 확인해봅시다.




```python
x_variable.data = y_tensor
y_variable.data = x_tensor

train()

x_test_data = np.linspace(-15, 15, n_samples)
x_test_tensor = torch.from_numpy(np.float32(x_test_data).reshape(n_samples, n_input))
x_test_variable.data = x_test_tensor

y_test_variable = network(x_test_variable)

# move from torch back to numpy
y_test_data = y_test_variable.data.numpy()

# plot the original data and the test data
plt.figure(figsize=(8, 8))
plt.scatter(y_data, x_data, alpha=0.2)
plt.scatter(x_test_data, y_test_data, alpha=0.2)
plt.show()
```

x_variable.data = y_tensor: 입력 변수 x를 나타내는 x_variable에 y_tensor 값을 할당합니다.

y_variable.data = x_tensor: 출력 변수 y를 나타내는 y_variable에 x_tensor 값을 할당합니다.

train(): 학습 함수를 호출하여 모델을 학습합니다.


x_test_data = np.linspace(-15, 15, n_samples): -15부터 15까지 n_samples 개수만큼의 데이터를 생성합니다.

x_test_tensor = torch.from_numpy(np.float32(x_test_data).reshape(n_samples, n_input)): 생성한 데이터를 numpy 배열에서 PyTorch 텐서로 변환합니다.

x_test_variable.data = x_test_tensor: 변환한 텐서를 입력 변수 x를 나타내는 x_test_variable에 할당합니다.

y_test_variable = network(x_test_variable): 학습된 모델을 사용하여 입력 변수 x_test_variable에 대한 출력 변수 y_test_variable 값을 계산합니다.

y_test_data = y_test_variable.data.numpy(): 계산된 출력 변수 y_test_variable 값을 numpy 배열로 변환합니다.

plt.scatter(y_data, x_data, alpha=0.2): 원래 데이터를 산점도로 시각화합니다.

plt.scatter(x_test_data, y_test_data, alpha=0.2): 예측된 데이터를 산점도로 시각화합니다.

plt.show(): 시각화 결과를 출력합니다.



### MDM
MDN은 Christopher Bishop이 제안한 구조입니다. 하나의 입력이 주어졌을 때 여러 개의 결과를 만들어낼 수 있는 방법이죠. 
곧 같은 x에 대해서 다른 분포를 따르는 y에서 p(y∣x)를 예측하는 것입니다. 

기존의 네트워크의 경우에는 하나의 입력에 대해 여러 개의 경우가 가능한 경우에 효과적으로 대응할 수 없었습니다. 
MDN은 이러한 문제를 해결하고자 등장했습니다. 
여러 개의 정규분포(혹은 다른 분포)의 평균과 분산을 예측하고, 각 정규분포에 속할 확률을 통해 이를 효과적으로 근사할 수 있게 한 것이죠.


```python

class MDN(nn.Module):
    def __init__(self, n_hidden, n_gaussians):
        super(MDN, self).__init__()
        self.z_h = nn.Sequential(
            nn.Linear(1, n_hidden),
            nn.Tanh()
        )
        self.z_pi = nn.Linear(n_hidden, n_gaussians)
        self.z_sigma = nn.Linear(n_hidden, n_gaussians)
        self.z_mu = nn.Linear(n_hidden, n_gaussians)  

    def forward(self, x):
        z_h = self.z_h(x)
        pi = nn.functional.softmax(self.z_pi(z_h), -1)
        sigma = torch.exp(self.z_sigma(z_h))
        mu = self.z_mu(z_h)
        return pi, sigma, mu

```

Mixture Density Network(MDN) 모델을 정의하는 클래스입니다.

def init(self, n_hidden, n_gaussians): 클래스 생성자 함수로, 은닉층의 뉴런 수(n_hidden)와 가우시안 분포의 수(n_gaussians)를 인자로 받습니다.

self.z_h = nn.Sequential(...): 입력 변수 x를 은닉층으로 전달하여 은닉층의 출력값을 계산하는 모듈입니다. 
nn.Linear와 nn.Tanh를 사용하여 구성되어 있습니다.


self.z_pi = nn.Linear(n_hidden, n_gaussians): 은닉층의 출력값을 입력으로 받아 가우시안 분포의 가중치(pi)를 계산하는 모듈입니다.

self.z_sigma = nn.Linear(n_hidden, n_gaussians): 은닉층의 출력값을 입력으로 받아 가우시안 분포의 표준편차(sigma)를 계산하는 모듈입니다.

self.z_mu = nn.Linear(n_hidden, n_gaussians): 은닉층의 출력값을 입력으로 받아 가우시안 분포의 평균(mu)을 계산하는 모듈입니다.

def forward(self, x): 입력 변수 x를 받아 MDN 모델의 출력값을 계산하는 함수입니다. 
먼저, 입력 변수 x를 은닉층(self.z_h)에 전달하여 은닉층의 출력값을 계산합니다. 
그 다음, 가중치(pi), 표준편차(sigma), 평균(mu)을 각각 계산합니다. 
마지막으로, 가중치(pi), 표준편차(sigma), 평균(mu)을 반환합니다.


zh 를 먼저 만들어내고, 이 값을 가지고 μ,σ,p를 만들어낼 겁니다. 
p는 확률의 정의를 만족해야 하므로 소프트맥스에 넣어줍니다.
 또한 
σ의 경우에는 양의 값을 가져야하므로 exponential 해줍시다.



\[N(\mu, \sigma)(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp (-\frac{(x-\mu)^2}{2\sigma^2})\]

```python
oneDivSqrtTwoPI = 1.0 / np.sqrt(2.0*np.pi)
def gaussian_distribution(y, mu, sigma):
    result = (y.expand_as(mu) - mu) * torch.reciprocal(sigma)
    result = -0.5 * (result * result)
    return (torch.exp(result) * torch.reciprocal(sigma)) * oneDivSqrtTwoPI
```

reciprocal의 경우에는 입력의 역수를 구해주는 함수입니다.


\[E = -\log{ \sum_{i=1}^m p(c = i \mid x)N(y; \mu^i, \sigma^i)}\]


```python

def mdn_loss_fn(pi, sigma, mu, y):
    result = gaussian_distribution(y, mu, sigma) * pi
    result = torch.sum(result, dim=1)
    result = -torch.log(result)
    return torch.mean(result)

```

Mixture Density Network(MDN) 모델의 손실 함수를 정의하는 함수입니다.

def mdn_loss_fn(pi, sigma, mu, y): 가중치(pi), 표준편차(sigma), 평균(mu), 출력 변수 y를 인자로 받습니다.

result = gaussian_distribution(y, mu, sigma) * pi: 출력 변수 y가 각각의 가우시안 분포에서 나올 확률을 계산합니다. 
이를 위해 gaussian_distribution 함수를 사용하여 각각의 가우시안 분포에서 y가 나올 확률을 계산하고, 가중치(pi)와 곱합니다.

result = torch.sum(result, dim=1): 각각의 가우시안 분포에서 y가 나올 확률의 합을 계산합니다.

result = -torch.log(result): 로그를 취한 후 음수로 변환하여 손실값을 계산합니다.

return torch.mean(result): 계산된 손실값의 평균을 반환합니다.


각 분포로부터 y가 나올 확률과 그 분포에 대응할 확률을 곱하고, 이들을 다 더한다음 로그와 평균을 취해주면 됩니다.


```python
network = MDN(n_hidden=20, n_gaussians=5)
optimizer = torch.optim.Adam(network.parameters())
```

여기서는 20개의 hidden layer unit과 5개의 정규분포를 사용해 학습을 시켜보겠습니다. 
optimizer로는 Adam을 사용합니다.

```python

mdn_x_data = y_data
mdn_y_data = x_data

mdn_x_tensor = y_tensor
mdn_y_tensor = x_tensor

x_variable = Variable(mdn_x_tensor)
y_variable = Variable(mdn_y_tensor, requires_grad=False)


def train_mdn():
    for epoch in range(10000):
        pi_variable, sigma_variable, mu_variable = network(x_variable)
        loss = mdn_loss_fn(pi_variable, sigma_variable, mu_variable, y_variable)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch % 500 == 0:
            print(epoch, loss.data[0])

train_mdn()
```

학습이 끝난 다음에는 입력의 변화에 따라 각 분포의 평균과 분산이 어떻게 달라지는지 확인할 수 있습니다


```python
pi_variable, sigma_variable, mu_variable = network(x_test_variable)

pi_data = pi_variable.data.numpy()
sigma_data = sigma_variable.data.numpy()
mu_data = mu_variable.data.numpy()

fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(8,8))
ax1.plot(x_test_data, pi_data)
ax1.set_title('$p(c = i | x)$')
ax2.plot(x_test_data, sigma_data)
ax2.set_title('$\sigma$')
ax3.plot(x_test_data, mu_data)
ax3.set_title('$\mu$')
plt.xlim([-15,15])
plt.show()
```
μ±σ 영역을 강조해서 영역을 그려볼 수도 있죠.


```python
plt.figure(figsize=(8, 8), facecolor='white')
for mu_k, sigma_k in zip(mu_data.T, sigma_data.T):
    plt.plot(x_test_data, mu_k)
    plt.fill_between(x_test_data, mu_k-sigma_k, mu_k+sigma_k, alpha=0.1)
plt.scatter(mdn_x_data, mdn_y_data, marker='.', lw=0, alpha=0.2, c='black')
plt.xlim([-10,10])
plt.ylim([-10,10])
plt.show()
```

matplotlib를 사용하여 MDN 모델의 결과를 시각화하는 코드입니다.

plt.figure(figsize=(8, 8), facecolor='white'): 새로운 그림을 생성합니다. 크기는 8x8이고, 배경색은 흰색입니다.

for mu_k, sigma_k in zip(mu_data.T, sigma_data.T): 각각의 가우시안 분포에 대해 평균(mu_k)과 표준편차(sigma_k)를 가져옵니다.

plt.plot(x_test_data, mu_k): x_test_data에 대한 mu_k를 그립니다.

plt.fill_between(x_test_data, mu_k-sigma_k, mu_k+sigma_k, alpha=0.1): x_test_data에 대한 mu_k의 표준편차(sigma_k) 범위를 색칠합니다.

plt.scatter(mdn_x_data, mdn_y_data, marker='.', lw=0, alpha=0.2, c='black'): 입력 데이터인 mdn_x_data와 출력 데이터인 mdn_y_data를 산점도로 그립니다.

plt.xlim([-10,10]): x축의 범위를 -10부터 10까지로 설정합니다.

plt.ylim([-10,10]): y축의 범위를 -10부터 10까지로 설정합니다.

plt.show(): 그림을 출력합니다.


한 x에 따라 여러개의 y가 가능할 수도 있음을 확인할 수도 있습니다. 
이들을 선택할 확률은 p(c=i ∣ x)에 의해서 결정되는 것입니다. 
더 많은 정규분포를 사용해서 loss를 줄일 수도 있겠지만, 결과를 해석하기는 더 어려워집니다.

학습시킨 네트워크에서 결과를 얻고 싶다면 특정한 정규분포를 하나 고르고 그로부터 값을 뽑아내야 합니다. 
이를 위해서는 Gumbel softmax sampling을 사용하면 됩니다.


```python
def gumbel_sample(x, axis=1):
    z = np.random.gumbel(loc=0, scale=1, size=x.shape)
    return (np.log(x) + z).argmax(axis=axis)

k = gumbel_sample(pi_data)
```

Gumbel-Softmax 샘플링을 수행하는 함수입니다.

def gumbel_sample(x, axis=1): 입력 변수 x와 축(axis)을 인자로 받습니다.

z = np.random.gumbel(loc=0, scale=1, size=x.shape): Gumbel 분포에서 샘플링한 값을 z에 저장합니다. 
loc은 분포의 위치 모수, scale은 분포의 스케일 모수입니다.

return (np.log(x) + z).argmax(axis=axis): 입력 변수 x에 로그를 취하고, Gumbel 분포에서 샘플링한 값을 더한 후, 
argmax 함수를 사용하여 가장 큰 값의 인덱스를 반환합니다. 
이를 통해 Gumbel-Softmax 샘플링을 수행합니다.

각 x에 대해서 어떤 정규분포를 선택해야되는지를 알았으니 각각의 평균과 분산을 이용해 이를 샘플링하기만 하면 됩니다.


```python
indices = (np.arange(n_samples), k)
rn = np.random.randn(n_samples)
sampled = rn * sigma_data[indices] + mu_data[indices]
```

rn의 경우에는 무작위의 노이즈이며 표준정규분포를 따르므로 이에 σ를 곱하고 μ를 더해주기만하면 원래의 정규분포를 얻을 수 있습니다. 


이렇게 해서 최종 결과물을 얻어보면 다음과 같습니다.

```python
plt.figure(figsize=(8, 8))
plt.scatter(mdn_x_data, mdn_y_data, alpha=0.2)
plt.scatter(x_test_data, sampled, alpha=0.2, color='red')
plt.show()
```
