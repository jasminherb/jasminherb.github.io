---
layout: single
title:  "로지스틱 회귀 & 회귀트리 with ChatGPT 코드설명 (24) "
---

# 로지스틱 회귀 & 회귀트리


## 로지스틱 회귀

```python
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer = load_breast_cancer()
```

해당 코드는 유방암 데이터를 불러오고, 필요한 라이브러리를 import하는 코드입니다.
pandas 라이브러리를 pd로 import합니다.

matplotlib.pyplot 라이브러리를 plt로 import합니다.

warnings 라이브러리를 import하고, 경고 메시지를 무시하도록 설정합니다.

%matplotlib inline 명령어를 사용하여 그래프를 주피터 노트북에 출력합니다.

sklearn.datasets 라이브러리에서 load_breast_cancer() 함수를 import합니다.

load_breast_cancer() 함수를 이용하여 유방암 데이터를 불러옵니다.

불러온 데이터를 cancer 변수에 저장합니다.

즉, 해당 코드는 유방암 데이터를 불러오고, 필요한 라이브러리를 import하는 코드입니다.


```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환
scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data)

X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)
```

해당 코드는 데이터를 전처리하고, 학습용 데이터와 테스트용 데이터를 분리하는 코드입니다.

sklearn.preprocessing 라이브러리에서 StandardScaler 클래스를 import합니다.

sklearn.model_selection 라이브러리에서 train_test_split 함수를 import합니다.

StandardScaler 객체를 생성합니다.

생성된 StandardScaler 객체를 이용하여 cancer.data를 표준화합니다. 표준화된 데이터는 data_scaled 변수에 저장됩니다.

train_test_split 함수를 이용하여 data_scaled와 cancer.target을 학습용 데이터와 테스트용 데이터로 분리합니다. 이 때, test_size는 0.3으로 설정하고, random_state는 0으로 설정합니다. 분리된 데이터는 각각 X_train, X_test, y_train, y_test 변수에 저장됩니다.


```python
from sklearn.metrics import accuracy_score, roc_auc_score

# 로지스틱 회귀를 이용하여 학습 및 예측 수행.
# solver 인자값을 생성자로 입력하지 않으면 solver='lbfgs'
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_preds = lr_clf.predict(X_test)

# accuracy와 roc_auc 측정
print('accuracy: {0:.3f}, roc_auc:{1:.3f}'.format(accuracy_score(y_test, lr_preds),
                                                  roc_auc_score(y_test , lr_preds)))
```

    accuracy: 0.977, roc_auc:0.972
    

해당 코드는 로지스틱 회귀 모델을 학습하고, 예측 결과를 출력하는 코드입니다.

sklearn.metrics 라이브러리에서 accuracy_score와 roc_auc_score 함수를 import합니다.

LogisticRegression 클래스를 이용하여 로지스틱 회귀 모델을 생성합니다. 이 때, solver 인자값을 생성자로 입력하지 않으면 solver='lbfgs'로 설정됩니다.

생성된 로지스틱 회귀 모델을 학습용 데이터(X_train, y_train)를 이용하여 학습시킵니다.

학습된 모델을 이용하여 테스트용 데이터(X_test)를 예측합니다. 예측 결과는 lr_preds 변수에 저장됩니다.

accuracy_score와 roc_auc_score 함수를 이용하여 예측 결과의 정확도와 ROC AUC 값을 출력합니다.


```python
solvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']

# 여러개의 solver 값별로 LogisticRegression 학습 후 성능 평가
for solver in solvers:
    lr_clf = LogisticRegression(solver=solver, max_iter=600)
    lr_clf.fit(X_train, y_train)
    lr_preds = lr_clf.predict(X_test)
    
    # accuracy와 roc_auc 측정
    print('solver:{0}, accuracy: {1:.3f}, roc_auc:{2:.3f}'.format(solver,
                                                                  accuracy_score(y_test, lr_preds),
                                                                  roc_auc_score(y_test , lr_preds)))
```

solver:lbfgs, accuracy: 0.977, roc_auc:0.972

solver:liblinear, accuracy: 0.982, roc_auc:0.979

solver:newton-cg, accuracy: 0.977, roc_auc:0.972

solver:sag, accuracy: 0.982, roc_auc:0.979

solver:saga, accuracy: 0.982, roc_auc:0.979





해당 코드는 로지스틱 회귀 모델의 solver 매개변수를 다양하게 설정하여 학습하고, 각각의 성능을 평가하는 코드입니다. solvers 리스트에는 다섯 가지의 solver 값이 저장되어 있습니다.
for 문을 이용하여 solvers 리스트에 저장된 모든 solver 값에 대해 다음과 같은 동작을 수행합니다.

LogisticRegression 객체를 생성합니다. 이 때 solver 매개변수는 for 문에서 현재 선택된 solver 값으로 설정됩니다.

생성된 LogisticRegression 객체를 X_train, y_train 데이터를 이용하여 학습합니다.

X_test 데이터를 이용하여 학습된 모델의 예측값을 계산합니다.

accuracy_score() 함수와 roc_auc_score() 함수를 이용하여 모델의 성능을 평가합니다. 이 때 y_test와 lr_preds를 인자로 전달합니다.

평가 결과를 출력합니다. 출력 내용은 solver 값, accuracy, roc_auc_score입니다.

즉, 해당 코드는 다양한 solver 값으로 로지스틱 회귀 모델을 학습하고, 각각의 성능을 평가하여 출력하는 코드입니다.


```python
from sklearn.model_selection import GridSearchCV

params={'solver':['liblinear', 'lbfgs'],
        'penalty':['l2', 'l1'],
        'C':[0.01, 0.1, 1, 1, 5, 10]}

lr_clf = LogisticRegression()

grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )
grid_clf.fit(data_scaled, cancer.target)
print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_,
                                                  grid_clf.best_score_))
```

    최적 하이퍼 파라미터:{'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}, 최적 평균 정확도:0.979
    

해당 코드는 그리드 서치를 이용하여 로지스틱 회귀 모델의 최적 하이퍼 파라미터를 찾는 코드입니다.

sklearn.model_selection 라이브러리에서 GridSearchCV 클래스를 import합니다.

하이퍼 파라미터 후보군을 params 딕셔너리에 저장합니다. solver, penalty, C 세 가지 파라미터를 조합하여 후보군을 만듭니다.

LogisticRegression 클래스를 이용하여 로지스틱 회귀 모델을 생성합니다.

GridSearchCV 클래스를 이용하여 그리드 서치 객체를 생성합니다. 이 때, 로지스틱 회귀 모델과 params 딕셔너리, scoring 방법, 교차 검증(cv) 횟수를 입력합니다.

생성된 그리드 서치 객체를 이용하여 데이터를 학습시킵니다.

최적 하이퍼 파라미터와 최적 평균 정확도를 출력합니다.

즉, 해당 코드는 그리드 서치를 이용하여 로지스틱 회귀 모델의 최적 하이퍼 파라미터를 찾고, 최적 평균 정확도를 출력하는 코드입니다.

## 5.8 회귀 트리


```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import numpy as np

# 보스턴 데이터 세트 로드
boston = load_boston()
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False)

rf = RandomForestRegressor(random_state=0, n_estimators=1000)
neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
rmse_scores  = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))
print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2))
print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))

```

     5 교차 검증의 개별 Negative MSE scores:  [ -7.88 -13.14 -20.57 -46.23 -18.88]
     5 교차 검증의 개별 RMSE scores :  [2.81 3.63 4.54 6.8  4.34]
     5 교차 검증의 평균 RMSE : 4.423 
    

해당 코드는 랜덤 포레스트 회귀 모델을 이용하여 보스턴 주택 가격 데이터 세트의 평균 제곱근 오차(RMSE)를 계산하는 코드입니다.

sklearn.datasets 라이브러리에서 load_boston 함수를 import합니다.

sklearn.model_selection 라이브러리에서 cross_val_score 함수를 import합니다.

sklearn.ensemble 라이브러리에서 RandomForestRegressor 클래스를 import합니다.

load_boston 함수를 이용하여 보스턴 주택 가격 데이터 세트를 로드합니다.

로드한 데이터를 pandas DataFrame으로 변환합니다.

y_target 변수에는 보스턴 주택 가격 데이터 세트의 target 값을 저장합니다.

X_data 변수에는 보스턴 주택 가격 데이터 세트의 feature 값을 저장합니다.

RandomForestRegressor 클래스를 이용하여 랜덤 포레스트 회귀 모델을 생성합니다. 이 때, random_state와 n_estimators를 입력합니다.

cross_val_score 함수를 이용하여 랜덤 포레스트 회귀 모델의 교차 검증을 수행합니다. 이 때, scoring 방법은 neg_mean_squared_error로 설정하고, cv는 5로 설정합니다.

교차 검증 결과인 neg_mse_scores를 이용하여 RMSE 값을 계산합니다.

계산된 RMSE 값을 이용하여 평균 RMSE 값을 출력합니다.

즉, 해당 코드는 랜덤 포레스트 회귀 모델을 이용하여 보스턴 주택 가격 데이터 세트의 평균 제곱근 오차(RMSE)를 계산하고, 결과를 출력하는 코드입니다.


```python
def get_model_cv_prediction(model, X_data, y_target):
    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
    rmse_scores  = np.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    print('##### ',model.__class__.__name__ , ' #####')
    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))
```

해당 코드는 모델과 데이터를 입력받아 교차 검증을 수행하고, 평균 RMSE 값을 출력하는 함수입니다.

함수의 인자로 모델, X_data, y_target을 입력받습니다.

cross_val_score 함수를 이용하여 모델의 교차 검증을 수행합니다. 이 때, scoring 방법은 neg_mean_squared_error로 설정하고, cv는 5로 설정합니다.

교차 검증 결과인 neg_mse_scores를 이용하여 RMSE 값을 계산합니다.

계산된 RMSE 값을 이용하여 평균 RMSE 값을 출력합니다.

즉, 해당 함수는 모델과 데이터를 입력받아 교차 검증을 수행하고, 평균 RMSE 값을 출력하는 함수입니다.


```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)
gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(n_estimators=1000)
lgb_reg = LGBMRegressor(n_estimators=1000)

# 트리 기반의 회귀 모델을 반복하면서 평가 수행 
models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]
for model in models:  
    get_model_cv_prediction(model, X_data, y_target)
```

    #####  DecisionTreeRegressor  #####
     5 교차 검증의 평균 RMSE : 5.978 
    #####  RandomForestRegressor  #####
     5 교차 검증의 평균 RMSE : 4.423 
    #####  GradientBoostingRegressor  #####
     5 교차 검증의 평균 RMSE : 4.269 
    #####  XGBRegressor  #####
     5 교차 검증의 평균 RMSE : 4.251 
    #####  LGBMRegressor  #####
     5 교차 검증의 평균 RMSE : 4.646 
    

해당 코드는 DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, LGBMRegressor 모델을 이용하여 보스턴 주택 가격 데이터 세트의 평균 제곱근 오차(RMSE)를 계산하는 코드입니다.

sklearn.tree 라이브러리에서 DecisionTreeRegressor 클래스를 import합니다.

sklearn.ensemble 라이브러리에서 GradientBoostingRegressor, RandomForestRegressor 클래스를 import합니다.

xgboost 라이브러리에서 XGBRegressor 클래스를 import합니다.

lightgbm 라이브러리에서 LGBMRegressor 클래스를 import합니다.

DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, LGBMRegressor 클래스를 이용하여 모델을 생성합니다. 이 때, 각 모델의 하이퍼 파라미터를 설정합니다.

생성한 모델들을 리스트에 저장합니다.

for문을 이용하여 리스트에 저장된 모델들을 반복하면서 get_model_cv_prediction 함수를 이용하여 교차 검증을 수행하고, 평균 RMSE 값을 출력합니다.

즉, 해당 코드는 DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, LGBMRegressor 모델을 이용하여 보스턴 주택 가격 데이터 세트의 평균 제곱근 오차(RMSE)를 계산하고, 결과를 출력하는 코드입니다.


```python
import seaborn as sns
%matplotlib inline

rf_reg = RandomForestRegressor(n_estimators=1000)

# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   
rf_reg.fit(X_data, y_target)

feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )
feature_series = feature_series.sort_values(ascending=False)
sns.barplot(x= feature_series, y=feature_series.index)
```




    <AxesSubplot:>




    
![png](output_18_1.png)
    


해당 코드는 RandomForestRegressor 모델을 이용하여 보스턴 주택 가격 데이터 세트의 특성 중요도를 시각화하는 코드입니다.

seaborn 라이브러리에서 sns를 import합니다.

RandomForestRegressor 모델을 생성합니다. 이 때, n_estimators를 1000으로 설정합니다.

생성한 RandomForestRegressor 모델을 이용하여 X_data, y_target 데이터 셋을 학습합니다.

학습된 모델의 feature_importances_ 속성을 이용하여 각 특성의 중요도를 계산합니다.

계산된 중요도를 Series 형태로 변환합니다. 이 때, index는 X_data.columns로 설정합니다.

중요도가 높은 순으로 정렬합니다.

seaborn의 barplot 함수를 이용하여 중요도가 높은 순으로 특성을 시각화합니다.

즉, 해당 코드는 RandomForestRegressor 모델을 이용하여 보스턴 주택 가격 데이터 세트의 특성 중요도를 계산하고, 시각화하는 코드입니다.


```python
import matplotlib.pyplot as plt
%matplotlib inline

bostonDF_sample = bostonDF[['RM','PRICE']]
bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0)
print(bostonDF_sample.shape)
plt.figure()
plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c="darkorange")
```

    (100, 2)
    




    <matplotlib.collections.PathCollection at 0x235ecd14580>




    
![png](output_20_2.png)
    


해당 코드는 보스턴 주택 가격 데이터 세트에서 RM과 PRICE 특성만을 선택하여 100개의 샘플을 무작위로 추출하고, 해당 샘플들의 산점도를 시각화하는 코드입니다.

matplotlib.pyplot 라이브러리에서 plt를 import합니다.

bostonDF에서 RM과 PRICE 특성만을 선택하여 bostonDF_sample 데이터 프레임을 생성합니다.

bostonDF_sample에서 n=100개의 샘플을 무작위로 추출합니다. 이 때, random_state를 0으로 설정합니다.

bostonDF_sample의 shape를 출력합니다.

plt.figure() 함수를 이용하여 새로운 figure를 생성합니다.

plt.scatter() 함수를 이용하여 bostonDF_sample의 RM 값을 x축으로, PRICE 값을 y축으로 설정하여 산점도를 시각화합니다. 이 때, 색상은 "darkorange"로 설정합니다.

즉, 해당 코드는 보스턴 주택 가격 데이터 세트에서 RM과 PRICE 특성만을 선택하여 100개의 샘플을 무작위로 추출하고, 해당 샘플들의 산점도를 시각화하는 코드입니다.


```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7
lr_reg = LinearRegression()
rf_reg2 = DecisionTreeRegressor(max_depth=2)
rf_reg7 = DecisionTreeRegressor(max_depth=7)

# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. 
X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)

# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출
X_feature = bostonDF_sample['RM'].values.reshape(-1,1)
y_target = bostonDF_sample['PRICE'].values.reshape(-1,1)

# 학습과 예측 수행. 
lr_reg.fit(X_feature, y_target)
rf_reg2.fit(X_feature, y_target)
rf_reg7.fit(X_feature, y_target)

pred_lr = lr_reg.predict(X_test)
pred_rf2 = rf_reg2.predict(X_test)
pred_rf7 = rf_reg7.predict(X_test)

```

해당 코드는 LinearRegression과 DecisionTreeRegressor(max_depth=2, 7) 모델을 이용하여 보스턴 주택 가격 데이터 세트에서 RM 특성만을 선택하여 학습하고, 테스트 데이터를 이용하여 예측하는 코드입니다.

numpy 라이브러리에서 np를 import합니다.

sklearn.linear_model 라이브러리에서 LinearRegression을 import합니다.

sklearn.tree 라이브러리에서 DecisionTreeRegressor를 import합니다.

LinearRegression과 DecisionTreeRegressor(max_depth=2, 7) 모델을 생성합니다.

예측을 수행할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋으로 생성합니다.

보스턴 주택가격 데이터에서 RM 특성만을 선택하여 X_feature 데이터를 생성합니다.

보스턴 주택가격 데이터에서 PRICE 특성만을 선택하여 y_target 데이터를 생성합니다.

LinearRegression, DecisionTreeRegressor(max_depth=2, 7) 모델을 이용하여 X_feature, y_target 데이터 셋을 학습합니다.

학습된 모델을 이용하여 X_test 데이터 셋에 대한 예측값을 계산합니다. 이 때, 각 모델의 예측값을 pred_lr, pred_rf2, pred_rf7 변수에 저장합니다.

즉, 해당 코드는 LinearRegression과 DecisionTreeRegressor(max_depth=2, 7) 모델을 이용하여 보스턴 주택 가격 데이터 세트에서 RM 특성만을 선택하여 학습하고, 테스트 데이터를 이용하여 예측하는 코드입니다.


```python
fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)

# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화
# 선형 회귀로 학습된 모델 회귀 예측선 
ax1.set_title('Linear Regression')
ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax1.plot(X_test, pred_lr,label="linear", linewidth=2 )

# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 
ax2.set_title('Decision Tree Regression: \n max_depth=2')
ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax2.plot(X_test, pred_rf2, label="max_depth:3", linewidth=2 )

# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 
ax3.set_title('Decision Tree Regression: \n max_depth=7')
ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax3.plot(X_test, pred_rf7, label="max_depth:7", linewidth=2)
```




    [<matplotlib.lines.Line2D at 0x1bb1a6ce940>]




    
![png](output_24_1.png)
    


해당 코드는 LinearRegression과 DecisionTreeRegressor(max_depth=2, 7) 모델을 이용하여 보스턴 주택 가격 데이터 세트에서 RM 특성만을 선택하여 학습하고, 테스트 데이터를 이용하여 예측한 결과를 시각화하는 코드입니다.



plt.subplots() 함수를 이용하여 1행 3열의 subplot을 생성합니다. 이 때, figsize는 (14,4)로 설정합니다.

각 subplot을 ax1, ax2, ax3 변수에 할당합니다.

ax1 subplot에 대해, 제목을 'Linear Regression'으로 설정하고, bostonDF_sample 데이터의 RM 값을 x축으로, PRICE 값을 y축으로 설정하여 산점도를 시각화합니다. 이후, LinearRegression 모델의 예측값인 pred_lr을 이용하여 회귀 예측선을 그립니다.

ax2 subplot에 대해, 제목을 'Decision Tree Regression: max_depth=2'으로 설정하고, bostonDF_sample 데이터의 RM 값을 x축으로, PRICE 값을 y축으로 설정하여 산점도를 시각화합니다. 이후, DecisionTreeRegressor(max_depth=2) 모델의 예측값인 pred_rf2을 이용하여 회귀 예측선을 그립니다.

ax3 subplot에 대해, 제목을 'Decision Tree Regression: max_depth=7'으로 설정하고, bostonDF_sample 데이터의 RM 값을 x축으로, PRICE 값을 y축으로 설정하여 산점도를 시각화합니다. 이후, DecisionTreeRegressor(max_depth=7) 모델의 예측값인 pred_rf7을 이용하여 회귀 예측선을 그립니다.


즉, 해당 코드는 LinearRegression과 DecisionTreeRegressor(max_depth=2, 7) 모델을 이용하여 보스턴 주택 가격 데이터 세트에서 RM 특성만을 선택하여 학습하고, 테스트 데이터를 이용하여 예측한 결과를 시각화하는 코드입니다.
