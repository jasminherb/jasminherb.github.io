---
layout: single
title:  "자연어 처리 - 문서분류 모델 (30) "
---


# NLP : 문서분류 모델

출처 : https://ratsgo.github.io/nlpbook

모델 환경 설정

```python

import torch

from ratsnlp.nlpbook.classification import ClassificationTrainArguments

args = ClassificationTrainArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_corpus_name="nsmc",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-doccls",
    batch_size=32 if torch.cuda.is_available() else 4,
    learning_rate=5e-5,
    max_seq_length=128,
    epochs=3,
    tpu_cores=0 if torch.cuda.is_available() else 8,
    seed=7,
)

```

pretrained_model_name : 프리트레인 마친 언어모델의 이름(단 해당 모델은 허깅페이스 라이브러리에 등록되어 있어야 합니다)

downstream_corpus_name : 다운스트림 데이터의 이름.

downstream_model_dir : 파인튜닝된 모델의 체크포인트가 저장될 위치.
 /gdrive/My Drive/nlpbook/checkpoint-doccls라고 함은 자신의 구글 드라이브의 내 폴더 하위의 nlpbook/checkpoint-doccls 디렉토리에 모델 체크포인트가 저장됩니다.

batch_size : 배치 크기. 하드웨어 가속기로 GPU를 선택(torch.cuda.is_available() == True)했다면 32, TPU라면(torch.cuda.is_available() == False) 4. 
코랩 환경에서 TPU는 보통 8개 코어가 할당되는데 batch_size는 코어별로 적용되는 배치 크기이기 때문에 이렇게 설정해 둡니다.

learning_rate : 러닝레이트. 1회 스텝에서 한 번에 얼마나 업데이트할지에 관한 크기를 가리킵니다. 

max_seq_length : 토큰 기준 입력 문장 최대 길이. 이보다 긴 문장은 max_seq_length로 자르고, 짧은 문장은 max_seq_length가 되도록 스페셜 토큰(PAD)을 붙여 줍니다.

epochs : 학습 에폭 수. 3이라면 학습 데이터를 3회 반복 학습합니다.

tpu_cores : TPU 코어 수. 하드웨어 가속기로 GPU를 선택(torch.cuda.is_available() == True)했다면 0, TPU라면(torch.cuda.is_available() == False) 8.

seed : 랜덤 시드(정수, integer). None을 입력하면 랜덤 시드를 고정하지 않습니다.


랜덤 시드(RANDOM SEED)
난수는 배치(batch)를 뽑거나 드롭아웃 대상 뉴런의 위치를 정할 때 등 다양하게 쓰입니다. 
컴퓨터는 난수 생성 알고리즘을 사용해 난수를 만들어내는데요. 
이 때 난수 생성 알고리즘을 실행하기 위해 쓰는 수를 랜덤 시드라고 합니다. 
만일 같은 시드를 사용한다면 컴퓨터는 계속 같은 패턴의 난수를 생성하게 됩니다.



랜덤 시드 고정

```python

from ratsnlp import nlpbook
nlpbook.set_seed(args)

```

각종 로그들을 출력하는 로거를 설정합니다.

```python

nlpbook.set_logger(args)
```

NSMC 데이터 다운로드를 수행합니다. 
데이터를 내려받는 도구로 코포라(Korpora)라는 오픈소스 파이썬 패키지를 사용해, 
corpus_name(nsmc)에 해당하는 말뭉치를 코랩 환경 로컬의 root_dir(/content/Korpora) 이하에 저장해 둡니다.

말뭉치 다운로드

```python

from Korpora import Korpora

Korpora.fetch(
    corpus_name=args.downstream_corpus_name,
    root_dir=args.downstream_corpus_root_dir,
    force_download=True,
)

```


데이터의 기본 단위는 텍스트 형태의 문장(sentence)입니다. 
토큰화(tokenization)란 문장을 토큰(token) 시퀀스로 분절하는 과정을 가리킵니다.

토크나이저(tokenizer)는 토큰화를 수행하는 프로그램이라는 뜻입니다.

토크나이저 준비

```python

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False,
)

```


딥러닝 모델을 학습하려면 학습데이터를 배치(batch) 단위로 지속적으로 모델에 공급해 주어야 합니다. 
파이토치(PyTorch)에서는 이 역할을 데이터 로더(DataLoader)가 수행하는데요.

NsmcCorpus는 CSV 파일 형식의 NSMC 데이터를 “문장(영화 리뷰) + 레이블(긍정, 부정)” 형태로 읽어들이는 역할을 하고요. 
ClassificationDataset는 데이터셋 역할을 수행합니다.

학습데이터셋 구축

```python

from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset

corpus = NsmcCorpus()
train_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="train",
)

```

NsmcCorpus가 넘겨준 데이터(문장, 레이블)를 모델이 학습할 수 있는 형태로 가공합니다. 
다시 말해 문장을 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수(integer)로 바꿔주는 역할을 합니다.


예컨대 NsmcCorpus가 넘겨준 데이터가 다음과 같다고 가정해 봅시다. 레이블 0은 부정(negative)이라는 뜻입니다.

text : 아 더빙.. 진짜 짜증나네요 목소리
label : 0(부정)


그러면 ClassificationDataset은 이를 다음과 같은 정보로 변환합니다. 
input_ids, attention_mask, token_type_ids의 길이가 모두 128인 이유는 토큰 기준 최대 길이(max_seq_length)를 args에서 128로 설정해 두었기 때문입니다.

input_ids에 패딩 토큰([PAD])의 인덱스에 해당하는 0이 많이 붙어 있음을 확인할 수 있습니다. 
분석 대상 문장의 토큰 길이가 max_seq_length보다 짧아서입니다. 이보다 긴 문장일 경우 128로 줄입니다.

attention_mask는 해당 토큰이 패딩 토큰인지(0) 아닌지(1)를 나타내며 token_type_ids는 세그먼트(segment) 정보로 기본값은 모두 0으로 넣습니다. 

label은 정수로 변환됐습니다.


input_ids : [2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

label : 0





학습데이터 로더구축

```python

from torch.utils.data import DataLoader, RandomSampler

train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=RandomSampler(train_dataset, replacement=False),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)

```

학습할 때 쓰이는 데이터 로더를 만들 수 있습니다. 
ClassificationDataset 클래스는 학습데이터에 속한 각각의 문장을 input_ids, attention_mask, token_type_ids, label 등 네 가지로 변환한 형태로 가지고 있습니다. 
데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배치 크기( args에서 정의한 batch_size)만큼을 뽑아 배치 형태로 가공하는 역할을 수행합니다


Sampler와 collate_fn이 눈에 띕니다. 

전자는 샘플링 방식을 정의합니다. 
데이터 로더는 배치를 만들 때 ClassificationDataset이 들고 있는 전체 인스턴스 가운데 batch_size 갯수만큼을 비복원(replacement=False) 랜덤 추출합니다.

후자는 이렇게 뽑힌 인스턴스를 배치로 만드는 역할을 하는 함수입니다. 
ClassificationDataset는 파이썬 리스트(list) 형태의 자료형인데요. 이를 파이토치가 요구하는 자료형인 텐서(tensor) 형태로 바꾸는 등의 역할을 수행합니다.


평가용 데이터 로더를 구축할 수 있습니다. 
학습용 데이터 로더와 달리 평가용 데이터 로더는 SequentialSampler를 사용하고 있음을 알 수 있습니다. 
SequentialSampler는 batch_size만큼의 갯수만큼을 인스턴스 순서대로 추출하는 역할을 합니다. 
학습 때 배치 구성은 랜덤으로 하는 것이 좋은데요. 
평가할 때는 평가용 데이터 전체를 사용하기 때문에 굳이 랜덤으로 구성할 이유가 없기 때문에 SequentialSampler를 씁니다.





평가용 데이터로더 구축

```python

from torch.utils.data import SequentialSampler

val_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="test",
)

val_dataloader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=SequentialSampler(val_dataset),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)

```

모델을 초기화합니다. BertForSequenceClassification은 프리트레인을 마친 BERT 모델 위에 "문서 분류용 태스크 모듈이 덧붙여진 형태의 모델 클래스"입니다. 
BertForSequenceClassification은 허깅페이스(huggingface)에서 제공하는 transformers 라이브러리에 포함돼 있습니다.

모델 초기화

```python

from transformers import BertConfig, BertForSequenceClassification

pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=corpus.num_labels,
)

model = BertForSequenceClassification.from_pretrained(
        args.pretrained_model_name,
        config=pretrained_model_config,
)

```

파이토치 라이트닝(pytorch lightning)이 제공하는 라이트닝 모듈(LightningModule) 클래스를 상속받아 태스크(task)를 정의합니다. 
태스크에는 모델과 옵티마이저(optimizer), 학습 과정(training process) 등이 정의돼 있습니다.



모델 클래스를 ClassificationTask에 포함시킵니다. 
ClassificationTask 클래스에는 옵티마이저, 러닝레이트 스케줄러(learning rate scheduler)가 정의되어 있는데요. 
옵티마이저로는 아담(Adam), 러닝레이트 스케줄러로는 ExponentialLR을 사용합니다.


TASK 정의

```python

from ratsnlp.nlpbook.classification import ClassificationTask
task = ClassificationTask(model, args)

```


Tip: 옵티마이저(OPTIMIZER)

옵티마이저란 최적화(optimization) 알고리즘을 가리킵니다. 아담(Adam)은 널리 쓰이는 옵티마이저 가운데 하나입니다. 

Tip: 러닝레이트 스케줄러(LEARNING RATE SCHEDULER)

모델 학습 과정은 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있습니다. 
러닝레이트는 한 번 내려갈 때 얼마나 이동할지 보폭에 해당합니다. 
학습이 진행되는 동안 점차 러닝레이트을 줄여 세밀하게 탐색하면 좀 더 좋은 모델을 만들 수 있습니다. 이 역할을 하는 게 바로 러닝레이트 스케줄러입니다. 
ExponentialLR은 현재 에폭의 러닝레이트를 이전 에폭의 러닝레이트  × gamma로 스케줄링합니다. 



트레이너는 파이토치 라이트닝 라이브러리의 도움을 받아 GPU/TPU 설정, 로그 및 체크포인트 등 귀찮은 설정들을 알아서 해줍니다.


TRAINER 정의

```python

trainer = nlpbook.get_trainer(args)

```





트레이너의 fit 함수를 호출하면 학습이 시작됩니다. 

학습 개시

```python

trainer.fit(
    task,
    train_dataloader=train_dataloader,
    val_dataloaders=val_dataloader,
)

```























학습 마친 모델을 실전 투입하기


학습을 마친 문서 분류 모델을 가지고 웹 서비스(web service)를 만들어보려고 합니다. 
문장을 받아 해당 문장이 긍정인지 부정인지 답변하는 웹 서비스인데요. 
문장을 토큰화한 뒤 모델 입력값으로 만들고 이를 모델에 태워 [해당 문장이 긍정일 확률, 해당 문장이 부정일 확률]을 계산하게 만듭니다. 
이후 약간의 후처리 과정을 거쳐 응답하게 만드는 방식입니다.

Tip: 웹 서비스(WEB SERVICE)
웹 서비스란 네트워크 상에서 컴퓨터들 간에 상호 작용을 하기 위해 만들어진 소프트웨어 시스템입니다. 
원격 사용자가 보낸 문장을 수신해 해당 문장이 긍정인지 부정인지 응답을 만들고 이 응답을 원격 사용자에게 송신하는 웹 서비스를 코랩 환경에 만듭니다.




인퍼런스 설정

```python

from ratsnlp.nlpbook.classification import ClassificationDeployArguments

args = ClassificationDeployArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-doccls",
    max_seq_length=128,
)

```


각 인자(argument)의 역할과 내용은 다음과 같습니다.

pretrained_model_name : 이전 장에서 파인튜닝한 모델이 사용한 프리트레인 마친 언어모델 이름(단 해당 모델은 허깅페이스 라이브러리에 등록되어 있어야 합니다)
downstream_model_dir : 이전 장에서 파인튜닝한 모델의 체크포인트 저장 위치.
max_seq_length : 토큰 기준 입력 문장 최대 길이. 아무 것도 입력하지 않으면 128입니다.



토크나이저 및 모델 불러오기
토크나이저를 초기화할 수 있습니다.

토크나이저 로드

```python

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False,
)

```


이전 장에서 파인튜닝한 모델의 체크포인트를 읽어들입니다.

체크포인트 로드

```python

import torch

fine_tuned_model_ckpt = torch.load(
    args.downstream_model_checkpoint_fpath,
    map_location=torch.device("cpu")
)



이전 장에서 파인튜닝한 모델이 사용한 프리트레인 마친 언어모델의 설정 값들을 읽어들일 수 있습니다. 

BERT 설정 로드

```python

from transformers import BertConfig

pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),
)

```



BERT 모델 초기화
해당 설정값대로 BERT 모델을 초기화합니다.

```python

from transformers import BertForSequenceClassification

model = BertForSequenceClassification(pretrained_model_config)

```




체크포인트 읽기
초기화한 BERT 모델에 체크포인트(fine_tuned_model_ckpt)를 읽어들이게 됩니다


```python

model.load_state_dict({k.replace("model.", ""): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})

```


평가 모드 전환
 모델이 평가 모드로 전환됩니다. 


```python

model.eval()

```

모델 출력값 만들고 후처리하기

인퍼런스 과정을 정의한 함수입니다. 문장(sentence)에 토큰화를 수행한 뒤 input_ids, attention_mask, token_type_ids를 만듭니다. 
이들 입력값을 파이토치 텐서(tensor) 자료형으로 변환한 뒤 모델에 입력합니다. 
모델 출력 값(outputs.logits)은 소프트맥스 함수 적용 이전의 로짓(logit) 형태인데요. 
여기에 소프트맥스 함수를 써서 모델 출력을 [부정일 확률, 긍정일 확률] 형태의 확률 형태로 바꿉니다.

마지막으로 모델 출력을 약간 후처리하여 예측 확률의 최댓값이 부정 위치일 경우 해당 문장이 부정(positive), 반대의 경우 긍정(positive)이 되도록 pred 값을 만듭니다.

INFERENCE

```python

def inference_fn(sentence):
    inputs = tokenizer(
        [sentence],
        max_length=args.max_seq_length,
        padding="max_length",
        truncation=True,
    )

    with torch.no_grad():
        outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()})
        prob = outputs.logits.softmax(dim=1)
        positive_prob = round(prob[0][1].item(), 4)
        negative_prob = round(prob[0][0].item(), 4)
        pred = "긍정 (positive)" if torch.argmax(prob) == 1 else "부정 (negative)"

    return {
        'sentence': sentence,
        'prediction': pred,
        'positive_data': f"긍정 {positive_prob}",
        'negative_data': f"부정 {negative_prob}",
        'positive_width': f"{positive_prob * 100}%",
        'negative_width': f"{negative_prob * 100}%",
    }

```

한편 positive_width, negative_width는  긍정/부정 막대 길이 조정을 위한 것으로 크게 신경쓰지 않아도 됩니다.




웹 서비스 시작하기
플라스크(flask)라는 파이썬 라이브러리의 도움을 받아 웹 서비스를 띄울 수 있습니다.

웹 서비스

```python

from ratsnlp.nlpbook.classification import get_web_service_app

app = get_web_service_app(inference_fn)
app.run()
'''


웹 브라우저로 http://37e862e6897f.ngrok.io에 접속하면  화면을 만날 수 있습니다.
단 실행할 때마다 이 주소가 변동하니 실제 접속할 때는 직접 코드를 실행해 당시 출력된 주소로 접근해야 합니다.
