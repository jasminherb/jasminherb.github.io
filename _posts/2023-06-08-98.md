---
layout: single
title:  "주성분 분석(PCA) (28) "
---


# Machine Learning - PCA

출처 : Thebook.io (모두의 딥러닝 & 머신러닝 워크북)

주성분 분석(Principal Component Analysis, PCA)은 다차원 데이터의 차원을 축소하는 방법으로, 
데이터의 가장 중요한 특징을 추출하는 데 사용됩니다. 
PCA는 데이터의 분산을 최대화하는 축을 찾아 그 축으로 데이터를 변환함으로써 차원을 축소합니다.


PCA의 작동 방식은 다음과 같습니다:

데이터 준비: PCA를 적용할 데이터를 준비합니다. 데이터는 수치형 변수로 이루어진 특징 벡터의 형태여야 합니다.

데이터 표준화: PCA는 데이터의 분산을 기반으로 작동하기 때문에, 데이터를 표준화하여 각 변수가 동일한 스케일을 갖도록 합니다. 
일반적으로 각 변수에서 평균을 빼고 표준편차로 나누어 표준화를 수행합니다.

공분산 행렬 계산: 표준화된 데이터를 기반으로 공분산 행렬을 계산합니다. 
공분산 행렬은 변수 간의 상관 관계를 나타내며, 주성분을 추출하는 데 사용됩니다.

고유값 분해: 공분산 행렬을 고유값 분해하여 고유값과 고유벡터를 구합니다. 
고유값은 주성분의 중요도를 나타내며, 고유벡터는 주성분의 방향을 나타냅니다.

주성분 선택: 고유값을 기준으로 주성분을 선택합니다. 
고유값이 클수록 해당 주성분의 중요도가 높습니다. 
원하는 차원 축소의 차원 수를 결정하여 해당 수만큼의 주성분을 선택합니다.

데이터 변환: 선택된 주성분으로 데이터를 변환합니다. 
변환된 데이터는 기존 데이터보다 저차원이며, 주성분에 대한 정보를 최대한 보존합니다.



PCA의 주요 장점:

차원 축소: PCA는 데이터의 차원을 축소하여 원래 데이터의 특징을 보존하면서 데이터를 더 낮은 차원으로 표현할 수 있습니다. 
이는 고차원 데이터의 시각화, 계산 효율성 향상, 잡음 제거 등 다양한 이점을 제공합니다.

데이터 압축: 차원 축소는 데이터 압축의 한 형태로 볼 수 있습니다. 
PCA는 데이터를 적은 수의 주성분으로 표현함으로써 데이터 크기를 줄일 수 있습니다. 
이는 데이터 저장 및 전송에 유리하며, 리소스 사용을 줄이는 데 도움이 됩니다.

특징 추출: PCA는 데이터의 주성분을 추출하므로, 데이터의 중요한 특징을 식별하는 데 사용될 수 있습니다. 
이는 패턴 인식, 분류, 회귀 등의 머신러닝 작업에서 유용합니다.

다중공선성 제거: PCA는 다중공선성(두 변수 간에 강한 상관 관계가 있는 경우)을 감소시키는 데 도움이 됩니다. 
다중공선성은 회귀 분석과 같은 모델링 작업에서 문제가 될 수 있으며, PCA는 변수 간의 상관 관계를 줄이고 모델의 안정성을 향상시킵니다.








PCA의 주요 단점:

정보 손실: PCA는 데이터를 낮은 차원으로 축소함으로써 정보 손실이 발생할 수 있습니다. 
낮은 차원으로 데이터를 표현하면서 원래 데이터의 변동성을 완벽하게 보존하지는 못합니다. 
따라서, 주성분의 설명력이 원본 데이터의 변동성을 충분히 설명하지 못할 수 있습니다.

선형 가정: PCA는 선형 변환을 사용하여 주성분을 추출하기 때문에, 비선형 관계를 잘 모델링하지 못합니다. 
비선형 데이터에 대해서는 다른 비선형 차원 축소 기법을 고려해야 할 수 있습니다.

계산 복잡성: PCA는 공분산 행렬의 고유값 분해를 계산해야 하므로, 큰 데이터셋에 대해서는 계산적으로 복잡할 수 있습니다. 
특히, 메모리 및 계산 리소스의 한계가 있는 경우에는 PCA의 사용이 제한될 수 있습니다.

해석의 어려움: PCA의 주성분은 해석에 어려움이 있습니다.

```python

from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

# 사이킷런 내장 데이터 셋 API 호출
iris = load_iris()

# 넘파이 데이터 셋을 Pandas DataFrame으로 변환
columns = ['sepal_length','sepal_width','petal_length','petal_width']
irisDF = pd.DataFrame(iris.data , columns=columns)
irisDF['target']=iris.target
irisDF.head(3)

#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현
markers=['^', 's', 'o']

#setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot 
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF['target']==i]['sepal_length']
    y_axis_data = irisDF[irisDF['target']==i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])

plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()

# Target 값을 제외한 모든 속성 값을 StandardScaler를 이용하여 표준 정규 분포를 가지는 값들로 변환
iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1])
print(iris_scaled.shape)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)

#fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)
print(iris_pca.shape)

# PCA  데이터의 컬럼명을 각각 pca_component_1, pca_component_2로 명명
pca_columns=['pca_component_1','pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns)
irisDF_pca['target']=iris.target
irisDF_pca.head(3)

#setosa를 세모, versicolor를 네모, virginica를 동그라미로 표시
markers=['^', 's', 'o']

#pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. 
for i, marker in enumerate(markers):
    x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1']
    y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2']
    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])

plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
plt.show()

print(pca.explained_variance_ratio_)
```
![23](https://github.com/jasminherb/jasminherb.github.io/assets/133365586/fa6a4223-da90-4543-a461-990d95b0f46b)
