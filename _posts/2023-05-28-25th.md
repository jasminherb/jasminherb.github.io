---
layout: single
title:  "XGBoost 구현하기(25) "
---

* XGBoost 버전 확인


```python
import xgboost

print(xgboost.__version__)
```

    1.7.3
    

### 파이썬 래퍼 XGBoost 적용 – 위스콘신 유방암 예측


```python
import xgboost as xgb
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()
X_features= dataset.data
y_label = dataset.target

cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_df['target']= y_label
cancer_df.head(3)

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.8</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.6</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.9</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.8</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.0</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.5</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 31 columns</p>
</div>




```python
print(dataset.target_names)
print(cancer_df['target'].value_counts())
```

    ['malignant' 'benign']
    1    357
    0    212
    Name: target, dtype: int64
    


```python
# cancer_df에서 feature용 DataFrame과 Label용 Series 객체 추출
# 맨 마지막 칼럼이 Label임. Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1 슬라이싱으로 추출.
X_features = cancer_df.iloc[:, :-1]
y_label = cancer_df.iloc[:, -1]

# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,
                                         test_size=0.2, random_state=156 )

# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리
X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )
print(X_train.shape , X_test.shape)
print(X_tr.shape, X_val.shape)
```

    (455, 30) (114, 30)
    (409, 30) (46, 30)
    


```python
# 만약 구버전 XGBoost에서 DataFrame으로 DMatrix 생성이 안될 경우 X_train.values로 넘파이 변환.
# 학습, 검증, 테스트용 DMatrix를 생성.
dtr = xgb.DMatrix(data=X_tr, label=y_tr)
dval = xgb.DMatrix(data=X_val, label=y_val)
dtest = xgb.DMatrix(data=X_test , label=y_test)
```


```python
params = { 'max_depth':3,
          'eta': 0.05,
          'objective':'binary:logistic',
          'eval_metric':'logloss'
         }
num_rounds = 400
```


```python
해당 코드는 XGBoost 모델의 하이퍼 파라미터를 설정하는 코드입니다.



max_depth는 3으로 설정합니다. 이는 트리의 최대 깊이를 의미합니다.

eta는 0.05로 설정합니다. 이는 학습률을 의미합니다.

objective는 'binary:logistic'으로 설정합니다. 이는 이진 분류 문제를 풀기 위한 로지스틱 회귀 모델을 사용한다는 것을 의미합니다.

eval_metric은 'logloss'로 설정합니다. 이는 모델의 성능을 평가하는 지표 중 하나인 로그 손실을 의미합니다.

num_rounds는 400으로 설정합니다. 이는 부스팅 반복 횟수를 의미합니다.


즉, 해당 코드는 XGBoost 모델의 하이퍼 파라미터를 설정하는 코드입니다.
```


```python
# 학습 데이터 셋은 'train' 또는 평가 데이터 셋은 'eval' 로 명기합니다. 
eval_list = [(dtr,'train'),(dval,'eval')] # 또는 eval_list = [(dval,'eval')] 만 명기해도 무방. 

# 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달
xgb_model = xgb.train(params = params , dtrain=dtr , num_boost_round=num_rounds , \
                      early_stopping_rounds=50, evals=eval_list )
```

    [0]	train-logloss:0.65016	eval-logloss:0.66183
    [1]	train-logloss:0.61131	eval-logloss:0.63609
    [2]	train-logloss:0.57563	eval-logloss:0.61144
    [3]	train-logloss:0.54310	eval-logloss:0.59204
    [4]	train-logloss:0.51323	eval-logloss:0.57329
    [5]	train-logloss:0.48447	eval-logloss:0.55037
    [6]	train-logloss:0.45796	eval-logloss:0.52930
    [7]	train-logloss:0.43436	eval-logloss:0.51534
    [8]	train-logloss:0.41150	eval-logloss:0.49718
    [9]	train-logloss:0.39027	eval-logloss:0.48154
    [10]	train-logloss:0.37128	eval-logloss:0.46990
    [11]	train-logloss:0.35254	eval-logloss:0.45474
    [12]	train-logloss:0.33528	eval-logloss:0.44229
    [13]	train-logloss:0.31892	eval-logloss:0.42961
    [14]	train-logloss:0.30439	eval-logloss:0.42065
    [15]	train-logloss:0.29000	eval-logloss:0.40958
    [16]	train-logloss:0.27651	eval-logloss:0.39887
    [17]	train-logloss:0.26389	eval-logloss:0.39050
    [18]	train-logloss:0.25210	eval-logloss:0.38254
    [19]	train-logloss:0.24123	eval-logloss:0.37393
    [20]	train-logloss:0.23076	eval-logloss:0.36789
    [21]	train-logloss:0.22091	eval-logloss:0.36017
    [22]	train-logloss:0.21155	eval-logloss:0.35421
    [23]	train-logloss:0.20263	eval-logloss:0.34683
    [24]	train-logloss:0.19434	eval-logloss:0.34111
    [25]	train-logloss:0.18637	eval-logloss:0.33634
    [26]	train-logloss:0.17875	eval-logloss:0.33082
    [27]	train-logloss:0.17167	eval-logloss:0.32675
    [28]	train-logloss:0.16481	eval-logloss:0.32099
    [29]	train-logloss:0.15835	eval-logloss:0.31671
    [30]	train-logloss:0.15225	eval-logloss:0.31277
    [31]	train-logloss:0.14650	eval-logloss:0.30882
    [32]	train-logloss:0.14102	eval-logloss:0.30437
    [33]	train-logloss:0.13590	eval-logloss:0.30103
    [34]	train-logloss:0.13109	eval-logloss:0.29794
    [35]	train-logloss:0.12647	eval-logloss:0.29499
    [36]	train-logloss:0.12197	eval-logloss:0.29295
    [37]	train-logloss:0.11784	eval-logloss:0.29043
    [38]	train-logloss:0.11379	eval-logloss:0.28927
    [39]	train-logloss:0.10994	eval-logloss:0.28578
    [40]	train-logloss:0.10638	eval-logloss:0.28364
    [41]	train-logloss:0.10302	eval-logloss:0.28183
    [42]	train-logloss:0.09963	eval-logloss:0.28005
    [43]	train-logloss:0.09649	eval-logloss:0.27972
    [44]	train-logloss:0.09359	eval-logloss:0.27744
    [45]	train-logloss:0.09080	eval-logloss:0.27542
    [46]	train-logloss:0.08807	eval-logloss:0.27504
    [47]	train-logloss:0.08541	eval-logloss:0.27458
    [48]	train-logloss:0.08299	eval-logloss:0.27348
    [49]	train-logloss:0.08035	eval-logloss:0.27247
    [50]	train-logloss:0.07786	eval-logloss:0.27163
    [51]	train-logloss:0.07550	eval-logloss:0.27094
    [52]	train-logloss:0.07344	eval-logloss:0.26967
    [53]	train-logloss:0.07147	eval-logloss:0.27008
    [54]	train-logloss:0.06964	eval-logloss:0.26890
    [55]	train-logloss:0.06766	eval-logloss:0.26854
    [56]	train-logloss:0.06591	eval-logloss:0.26900
    [57]	train-logloss:0.06433	eval-logloss:0.26790
    [58]	train-logloss:0.06259	eval-logloss:0.26663
    [59]	train-logloss:0.06107	eval-logloss:0.26743
    [60]	train-logloss:0.05957	eval-logloss:0.26610
    [61]	train-logloss:0.05817	eval-logloss:0.26644
    [62]	train-logloss:0.05691	eval-logloss:0.26673
    [63]	train-logloss:0.05550	eval-logloss:0.26550
    [64]	train-logloss:0.05422	eval-logloss:0.26443
    [65]	train-logloss:0.05311	eval-logloss:0.26500
    [66]	train-logloss:0.05207	eval-logloss:0.26591
    [67]	train-logloss:0.05093	eval-logloss:0.26501
    [68]	train-logloss:0.04976	eval-logloss:0.26435
    [69]	train-logloss:0.04872	eval-logloss:0.26360
    [70]	train-logloss:0.04776	eval-logloss:0.26319
    [71]	train-logloss:0.04680	eval-logloss:0.26255
    [72]	train-logloss:0.04580	eval-logloss:0.26204
    [73]	train-logloss:0.04484	eval-logloss:0.26254
    [74]	train-logloss:0.04388	eval-logloss:0.26289
    [75]	train-logloss:0.04309	eval-logloss:0.26249
    [76]	train-logloss:0.04224	eval-logloss:0.26217
    [77]	train-logloss:0.04133	eval-logloss:0.26166
    [78]	train-logloss:0.04050	eval-logloss:0.26179
    [79]	train-logloss:0.03967	eval-logloss:0.26103
    [80]	train-logloss:0.03876	eval-logloss:0.26094
    [81]	train-logloss:0.03806	eval-logloss:0.26148
    [82]	train-logloss:0.03740	eval-logloss:0.26054
    [83]	train-logloss:0.03676	eval-logloss:0.25967
    [84]	train-logloss:0.03605	eval-logloss:0.25905
    [85]	train-logloss:0.03545	eval-logloss:0.26007
    [86]	train-logloss:0.03489	eval-logloss:0.25984
    [87]	train-logloss:0.03425	eval-logloss:0.25933
    [88]	train-logloss:0.03361	eval-logloss:0.25932
    [89]	train-logloss:0.03311	eval-logloss:0.26002
    [90]	train-logloss:0.03260	eval-logloss:0.25936
    [91]	train-logloss:0.03202	eval-logloss:0.25886
    [92]	train-logloss:0.03152	eval-logloss:0.25918
    [93]	train-logloss:0.03107	eval-logloss:0.25864
    [94]	train-logloss:0.03049	eval-logloss:0.25951
    [95]	train-logloss:0.03007	eval-logloss:0.26091
    [96]	train-logloss:0.02963	eval-logloss:0.26014
    [97]	train-logloss:0.02913	eval-logloss:0.25974
    [98]	train-logloss:0.02866	eval-logloss:0.25937
    [99]	train-logloss:0.02829	eval-logloss:0.25893
    [100]	train-logloss:0.02789	eval-logloss:0.25928
    [101]	train-logloss:0.02751	eval-logloss:0.25955
    [102]	train-logloss:0.02714	eval-logloss:0.25901
    [103]	train-logloss:0.02668	eval-logloss:0.25991
    [104]	train-logloss:0.02634	eval-logloss:0.25950
    [105]	train-logloss:0.02594	eval-logloss:0.25924
    [106]	train-logloss:0.02556	eval-logloss:0.25901
    [107]	train-logloss:0.02522	eval-logloss:0.25738
    [108]	train-logloss:0.02492	eval-logloss:0.25702
    [109]	train-logloss:0.02453	eval-logloss:0.25789
    [110]	train-logloss:0.02418	eval-logloss:0.25770
    [111]	train-logloss:0.02384	eval-logloss:0.25842
    [112]	train-logloss:0.02356	eval-logloss:0.25810
    [113]	train-logloss:0.02322	eval-logloss:0.25848
    [114]	train-logloss:0.02290	eval-logloss:0.25833
    [115]	train-logloss:0.02260	eval-logloss:0.25820
    [116]	train-logloss:0.02229	eval-logloss:0.25905
    [117]	train-logloss:0.02204	eval-logloss:0.25878
    [118]	train-logloss:0.02176	eval-logloss:0.25728
    [119]	train-logloss:0.02149	eval-logloss:0.25722
    [120]	train-logloss:0.02119	eval-logloss:0.25764
    [121]	train-logloss:0.02095	eval-logloss:0.25761
    [122]	train-logloss:0.02067	eval-logloss:0.25832
    [123]	train-logloss:0.02045	eval-logloss:0.25808
    [124]	train-logloss:0.02023	eval-logloss:0.25855
    [125]	train-logloss:0.01998	eval-logloss:0.25714
    [126]	train-logloss:0.01973	eval-logloss:0.25587
    [127]	train-logloss:0.01946	eval-logloss:0.25640
    [128]	train-logloss:0.01927	eval-logloss:0.25685
    [129]	train-logloss:0.01908	eval-logloss:0.25665
    [130]	train-logloss:0.01886	eval-logloss:0.25712
    [131]	train-logloss:0.01863	eval-logloss:0.25609
    [132]	train-logloss:0.01839	eval-logloss:0.25649
    [133]	train-logloss:0.01816	eval-logloss:0.25789
    [134]	train-logloss:0.01802	eval-logloss:0.25811
    [135]	train-logloss:0.01785	eval-logloss:0.25794
    [136]	train-logloss:0.01763	eval-logloss:0.25876
    [137]	train-logloss:0.01748	eval-logloss:0.25884
    [138]	train-logloss:0.01732	eval-logloss:0.25867
    [139]	train-logloss:0.01719	eval-logloss:0.25876
    [140]	train-logloss:0.01696	eval-logloss:0.25987
    [141]	train-logloss:0.01681	eval-logloss:0.25960
    [142]	train-logloss:0.01669	eval-logloss:0.25982
    [143]	train-logloss:0.01656	eval-logloss:0.25992
    [144]	train-logloss:0.01638	eval-logloss:0.26035
    [145]	train-logloss:0.01623	eval-logloss:0.26055
    [146]	train-logloss:0.01606	eval-logloss:0.26092
    [147]	train-logloss:0.01589	eval-logloss:0.26137
    [148]	train-logloss:0.01572	eval-logloss:0.25999
    [149]	train-logloss:0.01556	eval-logloss:0.26028
    [150]	train-logloss:0.01546	eval-logloss:0.26048
    [151]	train-logloss:0.01531	eval-logloss:0.26142
    [152]	train-logloss:0.01515	eval-logloss:0.26188
    [153]	train-logloss:0.01501	eval-logloss:0.26227
    [154]	train-logloss:0.01486	eval-logloss:0.26287
    [155]	train-logloss:0.01476	eval-logloss:0.26299
    [156]	train-logloss:0.01462	eval-logloss:0.26346
    [157]	train-logloss:0.01448	eval-logloss:0.26379
    [158]	train-logloss:0.01434	eval-logloss:0.26306
    [159]	train-logloss:0.01424	eval-logloss:0.26237
    [160]	train-logloss:0.01410	eval-logloss:0.26251
    [161]	train-logloss:0.01401	eval-logloss:0.26265
    [162]	train-logloss:0.01392	eval-logloss:0.26264
    [163]	train-logloss:0.01380	eval-logloss:0.26250
    [164]	train-logloss:0.01372	eval-logloss:0.26264
    [165]	train-logloss:0.01359	eval-logloss:0.26255
    [166]	train-logloss:0.01350	eval-logloss:0.26188
    [167]	train-logloss:0.01342	eval-logloss:0.26203
    [168]	train-logloss:0.01331	eval-logloss:0.26190
    [169]	train-logloss:0.01319	eval-logloss:0.26184
    [170]	train-logloss:0.01312	eval-logloss:0.26133
    [171]	train-logloss:0.01304	eval-logloss:0.26148
    [172]	train-logloss:0.01297	eval-logloss:0.26157
    [173]	train-logloss:0.01285	eval-logloss:0.26253
    [174]	train-logloss:0.01278	eval-logloss:0.26229
    [175]	train-logloss:0.01267	eval-logloss:0.26086
    [176]	train-logloss:0.01258	eval-logloss:0.26103
    


```python
pred_probs = xgb_model.predict(dtest)
print('predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))

# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 
preds = [ 1 if x > 0.5 else 0 for x in pred_probs ]
print('예측값 10개만 표시:',preds[:10])
```

    predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨
    [0.845 0.008 0.68  0.081 0.975 0.999 0.998 0.998 0.996 0.001]
    예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
    

해당 코드는 XGBoost 모델을 이용하여 테스트 데이터에 대한 예측을 수행하고, 예측 결과를 출력하는 코드입니다.



xgb_model.predict(dtest)를 이용하여 테스트 데이터에 대한 예측 확률값을 pred_probs 변수에 저장합니다.

np.round(pred_probs[:10],3)를 이용하여 예측 확률값의 첫 10개를 소수점 셋째 자리까지 반올림하여 출력합니다.

preds = [ 1 if x > 0.5 else 0 for x in pred_probs ]를 이용하여 예측 확률값이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값을 결정하고, preds 변수에 저장합니다.

preds[:10]를 이용하여 예측값의 첫 10개를 출력합니다.


즉, 해당 코드는 XGBoost 모델을 이용하여 테스트 데이터에 대한 예측을 수행하고, 예측 결과를 출력하는 코드입니다.


```python
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```


```python
get_clf_eval(y_test , preds, pred_probs)
```

    오차 행렬
    [[34  3]
     [ 2 75]]
    정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9937
    


```python
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(xgb_model, ax=ax)
plt.savefig('p239_xgb_feature_importance.tif', format='tif', dpi=300, bbox_inches='tight')
```


    
![png](output_14_0.png)
    


해당 코드는 XGBoost 모델에서 각 feature의 중요도를 시각화하는 코드입니다.



plt.subplots(figsize=(10, 12))를 이용하여 그래프의 크기를 설정합니다.

plot_importance(xgb_model, ax=ax)를 이용하여 XGBoost 모델에서 feature의 중요도를 계산하고, 그래프로 시각화합니다.

plt.savefig()를 이용하여 그래프를 이미지 파일로 저장합니다.


즉, 해당 코드는 XGBoost 모델에서 각 feature의 중요도를 시각화하여 저장하는 코드입니다.

### 사이킷런 래퍼 XGBoost의 개요 및 적용


```python
# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트
from xgboost import XGBClassifier

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
xgb_wrapper.fit(X_train, y_train)
w_preds = xgb_wrapper.predict(X_test)
w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
```

해당 코드는 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier를 이용하여 분류 모델을 학습하고, 테스트 데이터에 대한 예측을 수행하는 코드입니다.



XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)를 이용하여 XGBoost 모델을 생성합니다. n_estimators는 트리의 개수, learning_rate는 학습률, max_depth는 트리의 최대 깊이를 의미합니다.

xgb_wrapper.fit(X_train, y_train)를 이용하여 XGBoost 모델을 학습합니다. X_train은 학습 데이터의 feature, y_train은 학습 데이터의 label을 의미합니다.

w_preds = xgb_wrapper.predict(X_test)를 이용하여 학습된 모델을 이용하여 테스트 데이터에 대한 예측값을 결정하고, w_preds 변수에 저장합니다.

w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]를 이용하여 테스트 데이터에 대한 예측 확률값을 결정하고, w_pred_proba 변수에 저장합니다.


즉, 해당 코드는 XGBClassifier를 이용하여 분류 모델을 학습하고, 테스트 데이터에 대한 예측과 예측 확률값을 저장하는 코드입니다.


```python
get_clf_eval(y_test , w_preds, w_pred_proba)
```

    오차 행렬
    [[35  2]
     [ 1 76]]
    정확도: 0.9737, 정밀도: 0.9744, 재현율: 0.9870,    F1: 0.9806, AUC:0.9951
    


```python
from xgboost import XGBClassifier

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
evals = [(X_test, y_test)]
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", 
                eval_set=evals, verbose=True)

ws100_preds = xgb_wrapper.predict(X_test)
ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
```

    [0]	validation_0-logloss:0.61352
    [1]	validation_0-logloss:0.54784
    [2]	validation_0-logloss:0.49425
    [3]	validation_0-logloss:0.44799
    [4]	validation_0-logloss:0.40911
    [5]	validation_0-logloss:0.37498
    [6]	validation_0-logloss:0.34571
    [7]	validation_0-logloss:0.32053
    [8]	validation_0-logloss:0.29721
    [9]	validation_0-logloss:0.27799
    [10]	validation_0-logloss:0.26030
    [11]	validation_0-logloss:0.24604
    [12]	validation_0-logloss:0.23156
    [13]	validation_0-logloss:0.22005
    [14]	validation_0-logloss:0.20857
    [15]	validation_0-logloss:0.19999
    [16]	validation_0-logloss:0.19012
    [17]	validation_0-logloss:0.18182
    [18]	validation_0-logloss:0.17473
    [19]	validation_0-logloss:0.16766
    [20]	validation_0-logloss:0.15820
    [21]	validation_0-logloss:0.15472
    [22]	validation_0-logloss:0.14895
    [23]	validation_0-logloss:0.14331
    [24]	validation_0-logloss:0.13634
    [25]	validation_0-logloss:0.13278
    [26]	validation_0-logloss:0.12791
    [27]	validation_0-logloss:0.12526
    [28]	validation_0-logloss:0.11998
    [29]	validation_0-logloss:0.11641
    [30]	validation_0-logloss:0.11450
    [31]	validation_0-logloss:0.11257
    [32]	validation_0-logloss:0.11154
    [33]	validation_0-logloss:0.10868
    [34]	validation_0-logloss:0.10668
    [35]	validation_0-logloss:0.10421
    [36]	validation_0-logloss:0.10296
    [37]	validation_0-logloss:0.10058
    [38]	validation_0-logloss:0.09868
    [39]	validation_0-logloss:0.09644
    [40]	validation_0-logloss:0.09587
    [41]	validation_0-logloss:0.09424
    [42]	validation_0-logloss:0.09471
    [43]	validation_0-logloss:0.09427
    [44]	validation_0-logloss:0.09389
    [45]	validation_0-logloss:0.09418
    [46]	validation_0-logloss:0.09402
    [47]	validation_0-logloss:0.09236
    [48]	validation_0-logloss:0.09301
    [49]	validation_0-logloss:0.09127
    [50]	validation_0-logloss:0.09005
    [51]	validation_0-logloss:0.08961
    [52]	validation_0-logloss:0.08958
    [53]	validation_0-logloss:0.09070
    [54]	validation_0-logloss:0.08958
    [55]	validation_0-logloss:0.09036
    [56]	validation_0-logloss:0.09159
    [57]	validation_0-logloss:0.09153
    [58]	validation_0-logloss:0.09199
    [59]	validation_0-logloss:0.09195
    [60]	validation_0-logloss:0.09194
    [61]	validation_0-logloss:0.09146
    [62]	validation_0-logloss:0.09031
    [63]	validation_0-logloss:0.08941
    [64]	validation_0-logloss:0.08972
    [65]	validation_0-logloss:0.08974
    [66]	validation_0-logloss:0.08962
    [67]	validation_0-logloss:0.08873
    [68]	validation_0-logloss:0.08862
    [69]	validation_0-logloss:0.08974
    [70]	validation_0-logloss:0.08998
    [71]	validation_0-logloss:0.08978
    [72]	validation_0-logloss:0.08958
    [73]	validation_0-logloss:0.08953
    [74]	validation_0-logloss:0.08875
    [75]	validation_0-logloss:0.08860
    [76]	validation_0-logloss:0.08812
    [77]	validation_0-logloss:0.08840
    [78]	validation_0-logloss:0.08874
    [79]	validation_0-logloss:0.08815
    [80]	validation_0-logloss:0.08758
    [81]	validation_0-logloss:0.08741
    [82]	validation_0-logloss:0.08849
    [83]	validation_0-logloss:0.08857
    [84]	validation_0-logloss:0.08807
    [85]	validation_0-logloss:0.08764
    [86]	validation_0-logloss:0.08742
    [87]	validation_0-logloss:0.08761
    [88]	validation_0-logloss:0.08707
    [89]	validation_0-logloss:0.08727
    [90]	validation_0-logloss:0.08716
    [91]	validation_0-logloss:0.08696
    [92]	validation_0-logloss:0.08717
    [93]	validation_0-logloss:0.08707
    [94]	validation_0-logloss:0.08659
    [95]	validation_0-logloss:0.08612
    [96]	validation_0-logloss:0.08714
    [97]	validation_0-logloss:0.08677
    [98]	validation_0-logloss:0.08669
    [99]	validation_0-logloss:0.08655
    [100]	validation_0-logloss:0.08650
    [101]	validation_0-logloss:0.08641
    [102]	validation_0-logloss:0.08629
    [103]	validation_0-logloss:0.08626
    [104]	validation_0-logloss:0.08683
    [105]	validation_0-logloss:0.08677
    [106]	validation_0-logloss:0.08732
    [107]	validation_0-logloss:0.08730
    [108]	validation_0-logloss:0.08728
    [109]	validation_0-logloss:0.08730
    [110]	validation_0-logloss:0.08729
    [111]	validation_0-logloss:0.08800
    [112]	validation_0-logloss:0.08794
    [113]	validation_0-logloss:0.08784
    [114]	validation_0-logloss:0.08807
    [115]	validation_0-logloss:0.08765
    [116]	validation_0-logloss:0.08730
    [117]	validation_0-logloss:0.08780
    [118]	validation_0-logloss:0.08775
    [119]	validation_0-logloss:0.08768
    [120]	validation_0-logloss:0.08763
    [121]	validation_0-logloss:0.08757
    [122]	validation_0-logloss:0.08755
    [123]	validation_0-logloss:0.08716
    [124]	validation_0-logloss:0.08767
    [125]	validation_0-logloss:0.08774
    [126]	validation_0-logloss:0.08827
    [127]	validation_0-logloss:0.08831
    [128]	validation_0-logloss:0.08827
    [129]	validation_0-logloss:0.08789
    [130]	validation_0-logloss:0.08886
    [131]	validation_0-logloss:0.08868
    [132]	validation_0-logloss:0.08874
    [133]	validation_0-logloss:0.08922
    [134]	validation_0-logloss:0.08918
    [135]	validation_0-logloss:0.08882
    [136]	validation_0-logloss:0.08851
    [137]	validation_0-logloss:0.08848
    [138]	validation_0-logloss:0.08839
    [139]	validation_0-logloss:0.08915
    [140]	validation_0-logloss:0.08911
    [141]	validation_0-logloss:0.08876
    [142]	validation_0-logloss:0.08868
    [143]	validation_0-logloss:0.08839
    [144]	validation_0-logloss:0.08927
    [145]	validation_0-logloss:0.08924
    [146]	validation_0-logloss:0.08914
    [147]	validation_0-logloss:0.08891
    [148]	validation_0-logloss:0.08942
    [149]	validation_0-logloss:0.08939
    [150]	validation_0-logloss:0.08911
    [151]	validation_0-logloss:0.08873
    [152]	validation_0-logloss:0.08872
    [153]	validation_0-logloss:0.08848
    [154]	validation_0-logloss:0.08847
    [155]	validation_0-logloss:0.08855
    [156]	validation_0-logloss:0.08852
    [157]	validation_0-logloss:0.08855
    [158]	validation_0-logloss:0.08827
    [159]	validation_0-logloss:0.08830
    [160]	validation_0-logloss:0.08828
    [161]	validation_0-logloss:0.08801
    [162]	validation_0-logloss:0.08776
    [163]	validation_0-logloss:0.08778
    [164]	validation_0-logloss:0.08778
    [165]	validation_0-logloss:0.08752
    [166]	validation_0-logloss:0.08754
    [167]	validation_0-logloss:0.08764
    [168]	validation_0-logloss:0.08739
    [169]	validation_0-logloss:0.08738
    [170]	validation_0-logloss:0.08730
    [171]	validation_0-logloss:0.08737
    [172]	validation_0-logloss:0.08740
    [173]	validation_0-logloss:0.08739
    [174]	validation_0-logloss:0.08713
    [175]	validation_0-logloss:0.08716
    [176]	validation_0-logloss:0.08695
    [177]	validation_0-logloss:0.08705
    [178]	validation_0-logloss:0.08697
    [179]	validation_0-logloss:0.08697
    [180]	validation_0-logloss:0.08704
    [181]	validation_0-logloss:0.08680
    [182]	validation_0-logloss:0.08683
    [183]	validation_0-logloss:0.08658
    [184]	validation_0-logloss:0.08659
    [185]	validation_0-logloss:0.08661
    [186]	validation_0-logloss:0.08637
    [187]	validation_0-logloss:0.08637
    [188]	validation_0-logloss:0.08630
    [189]	validation_0-logloss:0.08610
    [190]	validation_0-logloss:0.08602
    [191]	validation_0-logloss:0.08605
    [192]	validation_0-logloss:0.08615
    [193]	validation_0-logloss:0.08592
    [194]	validation_0-logloss:0.08591
    [195]	validation_0-logloss:0.08598
    [196]	validation_0-logloss:0.08601
    [197]	validation_0-logloss:0.08592
    [198]	validation_0-logloss:0.08585
    [199]	validation_0-logloss:0.08587
    [200]	validation_0-logloss:0.08589
    [201]	validation_0-logloss:0.08595
    [202]	validation_0-logloss:0.08573
    [203]	validation_0-logloss:0.08573
    [204]	validation_0-logloss:0.08575
    [205]	validation_0-logloss:0.08582
    [206]	validation_0-logloss:0.08584
    [207]	validation_0-logloss:0.08578
    [208]	validation_0-logloss:0.08569
    [209]	validation_0-logloss:0.08571
    [210]	validation_0-logloss:0.08581
    [211]	validation_0-logloss:0.08559
    [212]	validation_0-logloss:0.08580
    [213]	validation_0-logloss:0.08581
    [214]	validation_0-logloss:0.08574
    [215]	validation_0-logloss:0.08566
    [216]	validation_0-logloss:0.08584
    [217]	validation_0-logloss:0.08563
    [218]	validation_0-logloss:0.08573
    [219]	validation_0-logloss:0.08578
    [220]	validation_0-logloss:0.08579
    [221]	validation_0-logloss:0.08582
    [222]	validation_0-logloss:0.08576
    [223]	validation_0-logloss:0.08567
    [224]	validation_0-logloss:0.08586
    [225]	validation_0-logloss:0.08587
    [226]	validation_0-logloss:0.08593
    [227]	validation_0-logloss:0.08595
    [228]	validation_0-logloss:0.08587
    [229]	validation_0-logloss:0.08606
    [230]	validation_0-logloss:0.08600
    [231]	validation_0-logloss:0.08592
    [232]	validation_0-logloss:0.08610
    [233]	validation_0-logloss:0.08611
    [234]	validation_0-logloss:0.08617
    [235]	validation_0-logloss:0.08626
    [236]	validation_0-logloss:0.08629
    [237]	validation_0-logloss:0.08622
    [238]	validation_0-logloss:0.08639
    [239]	validation_0-logloss:0.08634
    [240]	validation_0-logloss:0.08618
    [241]	validation_0-logloss:0.08620
    [242]	validation_0-logloss:0.08625
    [243]	validation_0-logloss:0.08626
    [244]	validation_0-logloss:0.08629
    [245]	validation_0-logloss:0.08622
    [246]	validation_0-logloss:0.08640
    [247]	validation_0-logloss:0.08635
    [248]	validation_0-logloss:0.08628
    [249]	validation_0-logloss:0.08645
    [250]	validation_0-logloss:0.08629
    [251]	validation_0-logloss:0.08631
    [252]	validation_0-logloss:0.08636
    [253]	validation_0-logloss:0.08639
    [254]	validation_0-logloss:0.08649
    [255]	validation_0-logloss:0.08644
    [256]	validation_0-logloss:0.08629
    [257]	validation_0-logloss:0.08646
    [258]	validation_0-logloss:0.08639
    [259]	validation_0-logloss:0.08644
    [260]	validation_0-logloss:0.08646
    [261]	validation_0-logloss:0.08649
    [262]	validation_0-logloss:0.08645
    [263]	validation_0-logloss:0.08647
    [264]	validation_0-logloss:0.08632
    [265]	validation_0-logloss:0.08649
    [266]	validation_0-logloss:0.08654
    [267]	validation_0-logloss:0.08647
    [268]	validation_0-logloss:0.08650
    [269]	validation_0-logloss:0.08652
    [270]	validation_0-logloss:0.08669
    [271]	validation_0-logloss:0.08674
    [272]	validation_0-logloss:0.08683
    [273]	validation_0-logloss:0.08668
    [274]	validation_0-logloss:0.08664
    [275]	validation_0-logloss:0.08650
    [276]	validation_0-logloss:0.08635
    [277]	validation_0-logloss:0.08652
    [278]	validation_0-logloss:0.08657
    [279]	validation_0-logloss:0.08659
    [280]	validation_0-logloss:0.08668
    [281]	validation_0-logloss:0.08664
    [282]	validation_0-logloss:0.08650
    [283]	validation_0-logloss:0.08636
    [284]	validation_0-logloss:0.08640
    [285]	validation_0-logloss:0.08643
    [286]	validation_0-logloss:0.08646
    [287]	validation_0-logloss:0.08650
    [288]	validation_0-logloss:0.08637
    [289]	validation_0-logloss:0.08646
    [290]	validation_0-logloss:0.08645
    [291]	validation_0-logloss:0.08632
    [292]	validation_0-logloss:0.08628
    [293]	validation_0-logloss:0.08615
    [294]	validation_0-logloss:0.08620
    [295]	validation_0-logloss:0.08622
    [296]	validation_0-logloss:0.08631
    [297]	validation_0-logloss:0.08618
    [298]	validation_0-logloss:0.08626
    [299]	validation_0-logloss:0.08613
    [300]	validation_0-logloss:0.08618
    [301]	validation_0-logloss:0.08605
    [302]	validation_0-logloss:0.08602
    [303]	validation_0-logloss:0.08610
    [304]	validation_0-logloss:0.08598
    [305]	validation_0-logloss:0.08606
    [306]	validation_0-logloss:0.08597
    [307]	validation_0-logloss:0.08600
    [308]	validation_0-logloss:0.08600
    [309]	validation_0-logloss:0.08588
    [310]	validation_0-logloss:0.08592
    [311]	validation_0-logloss:0.08595
    


```python
get_clf_eval(y_test , ws100_preds, ws100_pred_proba)
```

    오차 행렬
    [[34  3]
     [ 1 76]]
    정확도: 0.9649, 정밀도: 0.9620, 재현율: 0.9870,    F1: 0.9744, AUC:0.9954
    


```python
# early_stopping_rounds를 10으로 설정하고 재 학습. 
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, 
                eval_metric="logloss", eval_set=evals,verbose=True)

ws10_preds = xgb_wrapper.predict(X_test)
ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
get_clf_eval(y_test , ws10_preds, ws10_pred_proba)
```

    [0]	validation_0-logloss:0.61352
    [1]	validation_0-logloss:0.54784
    [2]	validation_0-logloss:0.49425
    [3]	validation_0-logloss:0.44799
    [4]	validation_0-logloss:0.40911
    [5]	validation_0-logloss:0.37498
    [6]	validation_0-logloss:0.34571
    [7]	validation_0-logloss:0.32053
    [8]	validation_0-logloss:0.29721
    [9]	validation_0-logloss:0.27799
    [10]	validation_0-logloss:0.26030
    [11]	validation_0-logloss:0.24604
    [12]	validation_0-logloss:0.23156
    [13]	validation_0-logloss:0.22005
    [14]	validation_0-logloss:0.20857
    [15]	validation_0-logloss:0.19999
    [16]	validation_0-logloss:0.19012
    [17]	validation_0-logloss:0.18182
    [18]	validation_0-logloss:0.17473
    [19]	validation_0-logloss:0.16766
    [20]	validation_0-logloss:0.15820
    [21]	validation_0-logloss:0.15472
    [22]	validation_0-logloss:0.14895
    [23]	validation_0-logloss:0.14331
    [24]	validation_0-logloss:0.13634
    [25]	validation_0-logloss:0.13278
    [26]	validation_0-logloss:0.12791
    [27]	validation_0-logloss:0.12526
    [28]	validation_0-logloss:0.11998
    [29]	validation_0-logloss:0.11641
    [30]	validation_0-logloss:0.11450
    [31]	validation_0-logloss:0.11257
    [32]	validation_0-logloss:0.11154
    [33]	validation_0-logloss:0.10868
    [34]	validation_0-logloss:0.10668
    [35]	validation_0-logloss:0.10421
    [36]	validation_0-logloss:0.10296
    [37]	validation_0-logloss:0.10058
    [38]	validation_0-logloss:0.09868
    [39]	validation_0-logloss:0.09644
    [40]	validation_0-logloss:0.09587
    [41]	validation_0-logloss:0.09424
    [42]	validation_0-logloss:0.09471
    [43]	validation_0-logloss:0.09427
    [44]	validation_0-logloss:0.09389
    [45]	validation_0-logloss:0.09418
    [46]	validation_0-logloss:0.09402
    [47]	validation_0-logloss:0.09236
    [48]	validation_0-logloss:0.09301
    [49]	validation_0-logloss:0.09127
    [50]	validation_0-logloss:0.09005
    [51]	validation_0-logloss:0.08961
    [52]	validation_0-logloss:0.08958
    [53]	validation_0-logloss:0.09070
    [54]	validation_0-logloss:0.08958
    [55]	validation_0-logloss:0.09036
    [56]	validation_0-logloss:0.09159
    [57]	validation_0-logloss:0.09153
    [58]	validation_0-logloss:0.09199
    [59]	validation_0-logloss:0.09195
    [60]	validation_0-logloss:0.09194
    [61]	validation_0-logloss:0.09146
    오차 행렬
    [[34  3]
     [ 2 75]]
    정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9947
    


```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
# 사이킷런 래퍼 클래스를 입력해도 무방. 
plot_importance(xgb_wrapper, ax=ax)

```




    <AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>




    
![png](output_23_1.png)
    

