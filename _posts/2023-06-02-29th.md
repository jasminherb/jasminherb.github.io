---
layout: single
title:  "자연어 처리 기본내용  (29) "
---


# NLP

출처 : https://ratsgo.github.io/nlpbook

자연어 처리 모델은 자연어를 입력받아서,해당 입력이 특정 범주일 확률을 반환하는 확률 함수다.


딥러닝이란 데이터 패턴을 스스로 익히는 인공지능의 한 갈래입니다. 
여기에서 ‘딥deep’이란 많은 은닉층hidden layer을 사용한다는 의미입니다. 
딥러닝은 이미지 분류, 음성 인식 및 합성, 자연어 처리 등 다양한 분야에서 널리 쓰이고 있습니다.
딥러닝 가운데서도 BERT(Bidirectional Encoder Representations from Transformers)나 GPT (Generative Pre-trained Transformer)
등이 특히 주목받고 있습니다. 
이들을 딥러닝 기반 자연어 처리 모델이라고 부릅니다.





1.각종 설정값 정하기

모델을 만들려면 가장 먼저 각종 설정값을 정해야 합니다. 
어떤 프리트레인 모델을 사용할지,학습에 사용할 데이터는 무엇인지, 학습 결과는 어디에 저장할지 등이 바로 그것입니다. 
이 설정값들은 본격적인 학습에 앞서 미리 선언해 둡니다. 

하이퍼파라미터hyperparameter 역시 미리 정해둬야 하는 중요한 정보입니다. 
하이퍼파라미터란 모델 구조와 학습 등에 직접 관계된 설정값을 가리킵니다. 
예를 들어 러닝 레이트learning rate, 배치 크기batch size 등이 있습니다




2.데이터 내려받기

이 책에서는 프리트레인을 마친 모델을 다운스트림 데이터로 파인튜닝하는 실습을 진행합니다. 
파인튜닝을 하려면 다운스트림 데이터를 미리 내려받아 둬야 합니다. 



3.프리트레인을 마친 모델 준비하기

대규모 말뭉치를 활용한 프리트레인에는 많은 리소스가 필요합니다. 
다행히 최근 많은 기업과 개인이 프리트레인을 마친 모델을 자유롭게 사용할 수 있도록 공개하고 있어서 그 혜택을볼 수 있습니다.

특히 미국 자연어 처리 기업 허깅페이스huggingface에서 만든 트랜스포머(transformers)*라는
오픈소스 파이썬 패키지에 주목해야 합니다. 

이 책에서는 BERT, GPT 같은 트랜스포머 계열모델로 실습을 진행하는데, 
이 패키지를 쓰면 단 몇 줄만으로 모델을 사용할 수 있습니다.


4.토크나이저 준비하기

자연어 처리 모델의 입력은 대개 토큰token입니다. 
여기서 토큰이란 문장sentence보다 작은 단위입니다. 
한 문장은 여러 개의 토큰으로 구성됩니다. 
토큰 분리 기준은 그때그때 다를 수 있습니다. 
문장을 띄어쓰기만으로 나눌 수도 있고, 의미의 최소 단위인 형태소morpheme 단위로 나눌 수도 있습니다.

문장을 토큰 시퀀스token sequence로 분석하는 과정을 토큰화tokenization, 토큰화를 수행하는 프로그램을 토크나이저tokenizer라고 합니다. 
BPEByte Pair Encoding나 워드피스wordpiece 알고리즘을 채택한 토크나이저를 실습에 활용합니다. 


배치는 그 모양이 고정적이어야 할 때가 많습니다. 
다시 말해 동일한 배치에 있는 문장들의토큰(input_ids) 개수가 같아야 합니다. 
예를 들어 이번에 만들 배치가 데이터셋의 0번, 3번,6번 인스턴스이고 각각의 토큰 개수가 5, 3, 4개라고 가정해 보겠습니다. 
제일 긴 길이로 맞춘다면 0번 인스턴스의 길이(5개)에 따라 3번과 6번 인스턴스의 길이를 늘여야 합니다. 

이처럼 배치의 모양 등을 정비해 모델의 최종 입력으로 만들어 주는 과정을 컬레이트collate라고 합니다. 
컬레이트 과정에는 파이썬 리스트(list)에서 파이토치 텐서(tensor)로의 변환 등
자료형 변환도 포함됩니다.


파이토치 라이트닝이 제공하는 lightning 모듈을 상속받아 task를 정의합니다.
이 task에는 앞서 준비한 모델과 최적화 방법, 학습 과정 등이 정의돼 있습니다. 
최적화(optimization)란 특정 조건에서 어떤 값이 최대나 최소가 되도록 하는 과정을 가리킵니다. 
앞에서설명했던 것처럼 우리는 모델의 출력과 정답 사이의 차이를 작게 만드는 데 관심이 있습니다.
이를 위해 옵티마이저optimizer, 러닝 레이트 스케줄러learning rate scheduler 등을 정의해 둡니다.
모델 학습은 배치 단위로 이뤄집니다. 배치를 모델에 입력한 뒤 모델 출력을 정답과 비교해차이를 계산합니다. 
이후 그 차이를 최소화하는 방향으로 모델을 업데이트합니다. 
이 일련의순환 과정을 스텝step이라고 합니다.
Task의 학습 과정에는 1회 스텝에서 벌어지는 일들을 정의해 둡니다.
